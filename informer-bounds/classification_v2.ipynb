{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcae1152-8587-4463-ab97-0e5bc899cc23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('common_10s_20231112213000.csv', parse_dates=[\"date\"]).drop(['Unnamed: 0'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21a3c24-8adc-4562-8fd3-615c015c02fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20009502 entries, 0 to 20009501\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   ticker    object        \n",
      " 1   date      datetime64[ns]\n",
      " 2   open      float64       \n",
      " 3   high      float64       \n",
      " 4   low       float64       \n",
      " 5   close     float64       \n",
      " 6   average   float64       \n",
      " 7   volume    int64         \n",
      " 8   barcount  int64         \n",
      "dtypes: datetime64[ns](1), float64(5), int64(2), object(1)\n",
      "memory usage: 1.3+ GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>average</th>\n",
       "      <th>volume</th>\n",
       "      <th>barcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2023-09-01 09:30:00</td>\n",
       "      <td>453.17</td>\n",
       "      <td>453.20</td>\n",
       "      <td>452.89</td>\n",
       "      <td>452.90</td>\n",
       "      <td>453.064</td>\n",
       "      <td>397591</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2023-09-01 09:30:10</td>\n",
       "      <td>452.91</td>\n",
       "      <td>453.01</td>\n",
       "      <td>452.89</td>\n",
       "      <td>452.95</td>\n",
       "      <td>452.956</td>\n",
       "      <td>77825</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2023-09-01 09:30:20</td>\n",
       "      <td>452.95</td>\n",
       "      <td>453.09</td>\n",
       "      <td>452.95</td>\n",
       "      <td>453.02</td>\n",
       "      <td>453.034</td>\n",
       "      <td>47865</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2023-09-01 09:30:30</td>\n",
       "      <td>453.01</td>\n",
       "      <td>453.13</td>\n",
       "      <td>452.90</td>\n",
       "      <td>452.91</td>\n",
       "      <td>453.001</td>\n",
       "      <td>53428</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2023-09-01 09:30:40</td>\n",
       "      <td>452.92</td>\n",
       "      <td>453.10</td>\n",
       "      <td>452.91</td>\n",
       "      <td>453.03</td>\n",
       "      <td>452.981</td>\n",
       "      <td>65112</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker                date    open    high     low   close  average  volume  \\\n",
       "0    SPY 2023-09-01 09:30:00  453.17  453.20  452.89  452.90  453.064  397591   \n",
       "1    SPY 2023-09-01 09:30:10  452.91  453.01  452.89  452.95  452.956   77825   \n",
       "2    SPY 2023-09-01 09:30:20  452.95  453.09  452.95  453.02  453.034   47865   \n",
       "3    SPY 2023-09-01 09:30:30  453.01  453.13  452.90  452.91  453.001   53428   \n",
       "4    SPY 2023-09-01 09:30:40  452.92  453.10  452.91  453.03  452.981   65112   \n",
       "\n",
       "   barcount  \n",
       "0       944  \n",
       "1       584  \n",
       "2       312  \n",
       "3       408  \n",
       "4       423  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290abd48-21b7-4535-a9c4-b04ccf20f74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 55 dataframes grouped by tickers\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 363960 entries, 8268907 to 19377701\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count   Dtype         \n",
      "---  ------    --------------   -----         \n",
      " 0   date      363960 non-null  datetime64[ns]\n",
      " 1   open      363960 non-null  float64       \n",
      " 2   high      363960 non-null  float64       \n",
      " 3   low       363960 non-null  float64       \n",
      " 4   close     363960 non-null  float64       \n",
      " 5   average   363960 non-null  float64       \n",
      " 6   volume    363960 non-null  int64         \n",
      " 7   barcount  363960 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(5), int64(2)\n",
      "memory usage: 25.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>average</th>\n",
       "      <th>volume</th>\n",
       "      <th>barcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8268907</th>\n",
       "      <td>2023-03-27 09:30:00</td>\n",
       "      <td>13.96</td>\n",
       "      <td>14.03</td>\n",
       "      <td>13.95</td>\n",
       "      <td>14.00</td>\n",
       "      <td>13.978</td>\n",
       "      <td>246106</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268908</th>\n",
       "      <td>2023-03-27 09:30:10</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.015</td>\n",
       "      <td>11512</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268909</th>\n",
       "      <td>2023-03-27 09:30:20</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.018</td>\n",
       "      <td>13960</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268910</th>\n",
       "      <td>2023-03-27 09:30:30</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.013</td>\n",
       "      <td>18475</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268911</th>\n",
       "      <td>2023-03-27 09:30:40</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.020</td>\n",
       "      <td>32586</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date   open   high    low  close  average  volume  \\\n",
       "8268907 2023-03-27 09:30:00  13.96  14.03  13.95  14.00   13.978  246106   \n",
       "8268908 2023-03-27 09:30:10  14.00  14.02  14.00  14.02   14.015   11512   \n",
       "8268909 2023-03-27 09:30:20  14.02  14.02  14.01  14.02   14.018   13960   \n",
       "8268910 2023-03-27 09:30:30  14.02  14.03  14.01  14.01   14.013   18475   \n",
       "8268911 2023-03-27 09:30:40  14.02  14.03  14.00  14.03   14.020   32586   \n",
       "\n",
       "         barcount  \n",
       "8268907       158  \n",
       "8268908        23  \n",
       "8268909        23  \n",
       "8268910        93  \n",
       "8268911        63  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by ticker and remove the ticker label\n",
    "df_bytickers = [ticker.drop(columns=['ticker']) for _, ticker in df.groupby(df.ticker)]\n",
    "print(f\"There is {len(df_bytickers)} dataframes grouped by tickers\")\n",
    "df_bytickers[0].info()\n",
    "df_bytickers[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0813882e-4722-44da-b3b3-cc5e883f48a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is roughly 156 dataframes that correspond to days for each ticker\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2340 entries, 8268907 to 8271246\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   date      2340 non-null   float64\n",
      " 1   open      2340 non-null   float64\n",
      " 2   high      2340 non-null   float64\n",
      " 3   low       2340 non-null   float64\n",
      " 4   close     2340 non-null   float64\n",
      " 5   average   2340 non-null   float64\n",
      " 6   volume    2340 non-null   int64  \n",
      " 7   barcount  2340 non-null   int64  \n",
      "dtypes: float64(6), int64(2)\n",
      "memory usage: 164.5 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>average</th>\n",
       "      <th>volume</th>\n",
       "      <th>barcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8268907</th>\n",
       "      <td>9.500000</td>\n",
       "      <td>13.96</td>\n",
       "      <td>14.03</td>\n",
       "      <td>13.95</td>\n",
       "      <td>14.00</td>\n",
       "      <td>13.978</td>\n",
       "      <td>246106</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268908</th>\n",
       "      <td>9.502778</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.015</td>\n",
       "      <td>11512</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268909</th>\n",
       "      <td>9.505556</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.018</td>\n",
       "      <td>13960</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268910</th>\n",
       "      <td>9.508333</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.013</td>\n",
       "      <td>18475</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268911</th>\n",
       "      <td>9.511111</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.020</td>\n",
       "      <td>32586</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date   open   high    low  close  average  volume  barcount\n",
       "8268907  9.500000  13.96  14.03  13.95  14.00   13.978  246106       158\n",
       "8268908  9.502778  14.00  14.02  14.00  14.02   14.015   11512        23\n",
       "8268909  9.505556  14.02  14.02  14.01  14.02   14.018   13960        23\n",
       "8268910  9.508333  14.02  14.03  14.01  14.01   14.013   18475        93\n",
       "8268911  9.511111  14.02  14.03  14.00  14.03   14.020   32586        63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping by day, and reformatting the date to be time of day instead of datetime\n",
    "df_bydate = [[date for _, date in dates.groupby(dates['date'].dt.date)] for dates in df_bytickers]\n",
    "df_bydate = [[date.apply(lambda x: x.dt.hour + x.dt.minute/60 + x.dt.second/3600 if x.name in ['date'] else x) for date in ticker] for ticker in df_bydate]\n",
    "print(f\"There is roughly {len(df_bydate[0])} dataframes that correspond to days for each ticker\")\n",
    "df_bydate[0][0].info()\n",
    "df_bydate[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83962140-20aa-4e14-8389-3e5565fe56ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_with_zeros = df.eq(0).any()[lambda x: x].keys().values\n",
    "eps = 1e-16\n",
    "df_deltas = [[date.apply(lambda x: x + eps if x.name in columns_with_zeros else x) for date in ticker] for ticker in df_bydate]\n",
    "df_deltas = [[date.apply(lambda x: x.pct_change() if x.name not in ['date'] else x).iloc[1:] for date in ticker] for ticker in df_deltas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1492611-6947-45b7-8d60-676ade99c521",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2339 entries, 8268908 to 8271246\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   date      2339 non-null   float64\n",
      " 1   open      2339 non-null   float64\n",
      " 2   high      2339 non-null   float64\n",
      " 3   low       2339 non-null   float64\n",
      " 4   close     2339 non-null   float64\n",
      " 5   average   2339 non-null   float64\n",
      " 6   volume    2339 non-null   float64\n",
      " 7   barcount  2339 non-null   float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 164.5 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>average</th>\n",
       "      <th>volume</th>\n",
       "      <th>barcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8268908</th>\n",
       "      <td>9.502778</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>0.003584</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>-0.953223</td>\n",
       "      <td>-0.854430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268909</th>\n",
       "      <td>9.505556</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.212648</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268910</th>\n",
       "      <td>9.508333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.323424</td>\n",
       "      <td>3.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268911</th>\n",
       "      <td>9.511111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000714</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.763789</td>\n",
       "      <td>-0.322581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268912</th>\n",
       "      <td>9.513889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>-0.620236</td>\n",
       "      <td>-0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      open      high       low     close   average    volume  \\\n",
       "8268908  9.502778  0.002865 -0.000713  0.003584  0.001429  0.002647 -0.953223   \n",
       "8268909  9.505556  0.001429  0.000000  0.000714  0.000000  0.000214  0.212648   \n",
       "8268910  9.508333  0.000000  0.000713  0.000000 -0.000713 -0.000357  0.323424   \n",
       "8268911  9.511111  0.000000  0.000000 -0.000714  0.001428  0.000500  0.763789   \n",
       "8268912  9.513889  0.000000  0.000713  0.001429  0.000000  0.000428 -0.620236   \n",
       "\n",
       "         barcount  \n",
       "8268908 -0.854430  \n",
       "8268909  0.000000  \n",
       "8268910  3.043478  \n",
       "8268911 -0.322581  \n",
       "8268912 -0.142857  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deltas[0][0].info()\n",
    "df_deltas[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe9c54c-a536-463a-8822-198032ae1033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.892697979016721e-05\n",
      "['date' 'open' 'high' 'low' 'close' 'average' 'volume' 'barcount']\n"
     ]
    }
   ],
   "source": [
    "df_deltas = df_deltas\n",
    "#print(df_deltas[0][0].info())\n",
    "daily_sums = list()\n",
    "for tickers in df_deltas:\n",
    "    for days in tickers:\n",
    "        daily_sums.append(days.loc[:, 'average'].sum())\n",
    "daily_average = abs(sum(daily_sums) / len(daily_sums))\n",
    "features = df_deltas[0][0].columns.values\n",
    "print(daily_average)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e81210-532e-41b8-ba7e-8753e51f4c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "data = list(itertools.chain(*df_deltas))\n",
    "data_2339 = list()\n",
    "for v in data:\n",
    "    if (len(v) == 2339):\n",
    "        data_2339.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ccba49-297d-4efa-a7c3-cf5db5b06b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezxan/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torch/__init__.py:700: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "class StockDataset(TensorDataset):\n",
    "    def __init__(self, data, known_interval_in_tens_of_seconds=720, predict_interval_in_tens_of_seconds=180, daily_interval_in_tens_of_seconds=2339):\n",
    "        self.data = data\n",
    "        self.known_interval_in_tens_of_seconds = known_interval_in_tens_of_seconds\n",
    "        self.predict_interval_in_tens_of_seconds = predict_interval_in_tens_of_seconds\n",
    "        self.daily_length = daily_interval_in_tens_of_seconds - (known_interval_in_tens_of_seconds + predict_interval_in_tens_of_seconds)\n",
    "        self.length = len(data) * self.daily_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        list_idx = index // self.daily_length\n",
    "        df_idx = index % self.daily_length\n",
    "        known_df_idx = df_idx + self.known_interval_in_tens_of_seconds\n",
    "        predict_df_idx = known_df_idx + self.predict_interval_in_tens_of_seconds\n",
    "        #past_values = self.data[list_idx][['average']].iloc[df_idx:known_df_idx].values\n",
    "        #past_time_features = self.data[list_idx].loc[:, self.data[list_idx].columns != 'average'].iloc[df_idx:known_df_idx].values\n",
    "        #future_time_features = self.data[list_idx].loc[:, self.data[list_idx].columns != 'average'].iloc[known_df_idx:predict_df_idx].values\n",
    "        #future_values = self.data[list_idx][['average']].iloc[known_df_idx:predict_df_idx].values\n",
    "        #past_observed_mask = np.ones(past_values.shape)\n",
    "        return {\"past_values\": self.data[list_idx].loc[:, self.data[list_idx].columns != 'date'].iloc[df_idx:known_df_idx].values, \"future_values\": self.data[list_idx].loc[:, self.data[list_idx].columns != 'date'].iloc[known_df_idx:predict_df_idx].values, \n",
    "                \"past_time_features\": self.data[list_idx][['date']].iloc[df_idx:known_df_idx].values, \"future_time_features\": self.data[list_idx][['date']].iloc[known_df_idx:predict_df_idx].values}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04e512a7-ebd1-4949-a809-cd2732402f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12250207\n",
      "(720, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = StockDataset(data_2339)\n",
    "print(len(dataset))\n",
    "print(dataset[0]['past_time_features'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7935c738-101a-4a15-b4ec-fb8ff33e3f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds, eval_ds, test_ds, l = torch.utils.data.random_split(dataset, [0.4, 0.0005, 0.2, 0.3995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "901104a9-ad11-4b8d-b633-802d49b5ae73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezxan/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ezxan/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ezxan/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import InformerConfig, InformerForPrediction, Trainer, TrainingArguments, DefaultDataCollator\n",
    "from transformers.utils import is_sagemaker_mp_enabled\n",
    "from torch import nn\n",
    "\n",
    "model = InformerForPrediction.from_pretrained(\"forecasting_model_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf010c6-bc8c-428f-96ef-5916f94d652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "class BoundEstimatorConfig(PretrainedConfig):\n",
    "    model_type = \"informer\"\n",
    "    attribute_map = {\n",
    "        \"hidden_size\": \"d_model\",\n",
    "        \"num_attention_heads\": \"encoder_attention_heads\",\n",
    "        \"num_hidden_layers\": \"encoder_layers\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_length = None,\n",
    "        context_length = None,\n",
    "        distribution_output = \"student_t\",\n",
    "        loss = \"nll\",\n",
    "        input_size = 1,\n",
    "        lags_sequence = None,\n",
    "        scaling = \"mean\",\n",
    "        num_dynamic_real_features = 0,\n",
    "        num_static_real_features = 0,\n",
    "        num_static_categorical_features = 0,\n",
    "        num_time_features = 0,\n",
    "        cardinality = None,\n",
    "        embedding_dimension = None,\n",
    "        d_model = 64,\n",
    "        encoder_ffn_dim = 32,\n",
    "        encoder_attention_heads = 2,\n",
    "        encoder_layers = 2,\n",
    "        is_encoder_decoder = False,\n",
    "        activation_function  = \"gelu\",\n",
    "        dropout = 0.05,\n",
    "        encoder_layerdrop = 0.1,\n",
    "        attention_dropout = 0.1,\n",
    "        activation_dropout = 0.1,\n",
    "        num_parallel_samples = 100,\n",
    "        init_std = 0.02,\n",
    "        use_cache = True,\n",
    "        # Informer arguments\n",
    "        attention_type = \"prob\",\n",
    "        sampling_factor = 5,\n",
    "        distil = True,\n",
    "        num_labels = 3,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # time series specific configuration\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length or prediction_length\n",
    "        self.distribution_output = distribution_output\n",
    "        self.loss = loss\n",
    "        self.input_size = input_size\n",
    "        self.num_time_features = num_time_features\n",
    "        self.lags_sequence = lags_sequence if lags_sequence is not None else [1, 2, 3, 4, 5, 6, 7]\n",
    "        self.scaling = scaling\n",
    "        self.num_dynamic_real_features = num_dynamic_real_features\n",
    "        self.num_static_real_features = num_static_real_features\n",
    "        self.num_static_categorical_features = num_static_categorical_features\n",
    "\n",
    "        # set cardinality\n",
    "        if cardinality and num_static_categorical_features > 0:\n",
    "            if len(cardinality) != num_static_categorical_features:\n",
    "                raise ValueError(\n",
    "                    \"The cardinality should be a list of the same length as `num_static_categorical_features`\"\n",
    "                )\n",
    "            self.cardinality = cardinality\n",
    "        else:\n",
    "            self.cardinality = [0]\n",
    "\n",
    "        # set embedding_dimension\n",
    "        if embedding_dimension and num_static_categorical_features > 0:\n",
    "            if len(embedding_dimension) != num_static_categorical_features:\n",
    "                raise ValueError(\n",
    "                    \"The embedding dimension should be a list of the same length as `num_static_categorical_features`\"\n",
    "                )\n",
    "            self.embedding_dimension = embedding_dimension\n",
    "        else:\n",
    "            self.embedding_dimension = [min(50, (cat + 1) // 2) for cat in self.cardinality]\n",
    "\n",
    "        self.num_parallel_samples = num_parallel_samples\n",
    "\n",
    "        # Transformer architecture configuration\n",
    "        self.feature_size = input_size * len(self.lags_sequence) + self._number_of_features\n",
    "        self.d_model = d_model\n",
    "        self.encoder_attention_heads = encoder_attention_heads\n",
    "        self.encoder_ffn_dim = encoder_ffn_dim\n",
    "        self.encoder_layers = encoder_layers\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.encoder_layerdrop = encoder_layerdrop\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.init_std = init_std\n",
    "\n",
    "        self.output_attentions = False\n",
    "        self.output_hidden_states = False\n",
    "\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        # Informer\n",
    "        self.attention_type = attention_type\n",
    "        self.sampling_factor = sampling_factor\n",
    "        self.distil = distil\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n",
    "        \n",
    "    @property\n",
    "    def _number_of_features(self) -> int:\n",
    "        return (\n",
    "            sum(self.embedding_dimension)\n",
    "            + self.num_dynamic_real_features\n",
    "            + self.num_time_features\n",
    "            + self.num_static_real_features\n",
    "            + self.input_size * 2  # the log1p(abs(loc)) and log(scale) features\n",
    "        )\n",
    "    def from_informer_config(self, config):\n",
    "        # time series specific configuration\n",
    "        self.prediction_length = config.prediction_length\n",
    "        self.context_length = config.context_length\n",
    "        self.distribution_output = config.distribution_output\n",
    "        self.loss = config.loss\n",
    "        self.input_size = config.input_size\n",
    "        self.num_time_features = config.num_time_features\n",
    "        self.lags_sequence = config.lags_sequence\n",
    "        self.scaling = config.scaling\n",
    "        self.num_dynamic_real_features = config.num_dynamic_real_features\n",
    "        self.num_static_real_features = config.num_static_real_features\n",
    "        self.num_static_categorical_features = config.num_static_categorical_features\n",
    "\n",
    "        # set cardinality\n",
    "        self.cardinality = config.cardinality\n",
    "\n",
    "        # set embedding_dimension\n",
    "        self.embedding_dimension = config.embedding_dimension\n",
    "\n",
    "        self.num_parallel_samples = config.num_parallel_samples\n",
    "\n",
    "        # Transformer architecture configuration\n",
    "        self.feature_size = config.feature_size\n",
    "        self.d_model = config.d_model\n",
    "        self.encoder_attention_heads = config.encoder_attention_heads\n",
    "        self.encoder_ffn_dim = config.encoder_ffn_dim\n",
    "        self.encoder_layers = config.encoder_layers\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "        self.encoder_layerdrop = config.encoder_layerdrop\n",
    "\n",
    "        self.activation_function = config.activation_function\n",
    "        self.init_std = config.init_std\n",
    "\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_attentions\n",
    "\n",
    "        self.use_cache = config.use_cache\n",
    "\n",
    "        # Informer\n",
    "        self.attention_type = config.attention_type\n",
    "        self.sampling_factor = config.sampling_factor\n",
    "        self.distil = config.distil\n",
    "        self.num_labels = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd727250-9409-490f-adc7-142e1ef410eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "class InformerFeatureEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed a sequence of categorical features.\n",
    "    Args: \n",
    "        cardinalities (`list[int]`):\n",
    "            List of cardinalities of the categorical features.\n",
    "        embedding_dims (`list[int]`):\n",
    "            List of embedding dimensions of the categorical features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cardinalities: List[int], embedding_dims: List[int]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = len(cardinalities)\n",
    "        self.embedders = nn.ModuleList([nn.Embedding(c, d) for c, d in zip(cardinalities, embedding_dims)])\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.num_features > 1:\n",
    "            # we slice the last dimension, giving an array of length\n",
    "            # self.num_features with shape (N,T) or (N)\n",
    "            cat_feature_slices = torch.chunk(features, self.num_features, dim=-1)\n",
    "        else:\n",
    "            cat_feature_slices = [features]\n",
    "\n",
    "        return torch.cat(\n",
    "            [\n",
    "                embed(cat_feature_slice.squeeze(-1))\n",
    "                for embed, cat_feature_slice in zip(self.embedders, cat_feature_slices)\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesStdScaler with TimeSeries->Informer\n",
    "class InformerStdScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Standardize features by calculating the mean and scaling along some given dimension `dim`, and then normalizes it\n",
    "    by subtracting from the mean and dividing by the standard deviation.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to calculate the mean and standard deviation.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "        minimum_scale (`float`, *optional*, defaults to 1e-5):\n",
    "            Default scale that is used for elements that are constantly zero along dimension `dim`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, keepdim: bool = False, minimum_scale: float = 1e-5):\n",
    "        super().__init__()\n",
    "        if not dim > 0:\n",
    "            raise ValueError(\"Cannot compute scale along dim = 0 (batch dimension), please provide dim > 0\")\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "        self.minimum_scale = minimum_scale\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: torch.Tensor, weights: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        denominator = weights.sum(self.dim, keepdim=self.keepdim)\n",
    "        denominator = denominator.clamp_min(1.0)\n",
    "        loc = (data * weights).sum(self.dim, keepdim=self.keepdim) / denominator\n",
    "\n",
    "        variance = (((data - loc) * weights) ** 2).sum(self.dim, keepdim=self.keepdim) / denominator\n",
    "        scale = torch.sqrt(variance + self.minimum_scale)\n",
    "        return (data - loc) / scale, loc, scale\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesMeanScaler with TimeSeries->Informer\n",
    "class InformerMeanScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a scaling factor as the weighted average absolute value along dimension `dim`, and scales the data\n",
    "    accordingly.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to compute the scale.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "        default_scale (`float`, *optional*, defaults to `None`):\n",
    "            Default scale that is used for elements that are constantly zero. If `None`, we use the scale of the batch.\n",
    "        minimum_scale (`float`, *optional*, defaults to 1e-10):\n",
    "            Default minimum possible scale that is used for any item.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dim: int = -1, keepdim: bool = True, default_scale: Optional[float] = None, minimum_scale: float = 1e-10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "        self.minimum_scale = minimum_scale\n",
    "        self.default_scale = default_scale\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: torch.Tensor, observed_indicator: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # shape: (N, [C], T=1)\n",
    "        ts_sum = (data * observed_indicator).abs().sum(self.dim, keepdim=True)\n",
    "        num_observed = observed_indicator.sum(self.dim, keepdim=True)\n",
    "\n",
    "        scale = ts_sum / torch.clamp(num_observed, min=1)\n",
    "\n",
    "        # If `default_scale` is provided, we use it, otherwise we use the scale\n",
    "        # of the batch.\n",
    "        if self.default_scale is None:\n",
    "            batch_sum = ts_sum.sum(dim=0)\n",
    "            batch_observations = torch.clamp(num_observed.sum(0), min=1)\n",
    "            default_scale = torch.squeeze(batch_sum / batch_observations)\n",
    "        else:\n",
    "            default_scale = self.default_scale * torch.ones_like(scale)\n",
    "\n",
    "        # apply default scale where there are no observations\n",
    "        scale = torch.where(num_observed > 0, scale, default_scale)\n",
    "\n",
    "        # ensure the scale is at least `self.minimum_scale`\n",
    "        scale = torch.clamp(scale, min=self.minimum_scale)\n",
    "        scaled_data = data / scale\n",
    "\n",
    "        if not self.keepdim:\n",
    "            scale = scale.squeeze(dim=self.dim)\n",
    "\n",
    "        return scaled_data, torch.zeros_like(scale), scale\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesNOPScaler with TimeSeries->Informer\n",
    "class InformerNOPScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Assigns a scaling factor equal to 1 along dimension `dim`, and therefore applies no scaling to the input data.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to compute the scale.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, keepdim: bool = False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "\n",
    "    def forward(\n",
    "        self, data: torch.Tensor, observed_indicator: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        scale = torch.ones_like(data, requires_grad=False).mean(dim=self.dim, keepdim=self.keepdim)\n",
    "        loc = torch.zeros_like(data, requires_grad=False).mean(dim=self.dim, keepdim=self.keepdim)\n",
    "        return data, loc, scale\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.weighted_average\n",
    "def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the weighted average of a given tensor across a given `dim`, masking values associated with weight zero,\n",
    "    meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n",
    "    Args:\n",
    "        input_tensor (`torch.FloatTensor`):\n",
    "            Input tensor, of which the average must be computed.\n",
    "        weights (`torch.FloatTensor`, *optional*):\n",
    "            Weights tensor, of the same shape as `input_tensor`.\n",
    "        dim (`int`, *optional*):\n",
    "            The dim along which to average `input_tensor`.\n",
    "    Returns:\n",
    "        `torch.FloatTensor`: The tensor with values averaged along the specified `dim`.\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        weighted_tensor = torch.where(weights != 0, input_tensor * weights, torch.zeros_like(input_tensor))\n",
    "        sum_weights = torch.clamp(weights.sum(dim=dim) if dim else weights.sum(), min=1.0)\n",
    "        return (weighted_tensor.sum(dim=dim) if dim else weighted_tensor.sum()) / sum_weights\n",
    "    else:\n",
    "        return input_tensor.mean(dim=dim)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.nll\n",
    "def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the negative log likelihood loss from input distribution with respect to target.\n",
    "    \"\"\"\n",
    "    return -input.log_prob(target)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart._make_causal_mask\n",
    "def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n",
    "    \"\"\"\n",
    "    Make causal mask used for bi-directional self-attention.\n",
    "    \"\"\"\n",
    "    bsz, tgt_len = input_ids_shape\n",
    "    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
    "    mask_cond = torch.arange(mask.size(-1))\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "    mask = mask.to(dtype)\n",
    "\n",
    "    if past_key_values_length > 0:\n",
    "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n",
    "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart._expand_mask\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian->Informer\n",
    "class InformerSinusoidalPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
    "\n",
    "    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n",
    "        super().__init__(num_positions, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weight(out: nn.Parameter) -> nn.Parameter:\n",
    "        \"\"\"\n",
    "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
    "        the 2nd half of the vector. [dim // 2:]\n",
    "        \"\"\"\n",
    "        n_pos, dim = out.shape\n",
    "        position_enc = np.array(\n",
    "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
    "        )\n",
    "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
    "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
    "        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "        out.detach_()\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n",
    "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = input_ids_shape[:2]\n",
    "        positions = torch.arange(\n",
    "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
    "        )\n",
    "        return super().forward(positions)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesValueEmbedding with TimeSeries->Info\n",
    "class InformerValueEmbedding(nn.Module):\n",
    "    def __init__(self, feature_size, d_model):\n",
    "        super().__init__()\n",
    "        self.value_projection = nn.Linear(in_features=feature_size, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.value_projection(x)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Informer\n",
    "class InformerAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "        # the provided `key_value_states` to support prefix tuning\n",
    "        if (\n",
    "            is_cross_attention\n",
    "            and past_key_value is not None\n",
    "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "        ):\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.reshape(*proj_shape)\n",
    "        value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned across GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value\n",
    "\n",
    "\n",
    "class InformerProbSparseAttention(nn.Module):\n",
    "    \"\"\"Probabilistic Attention mechanism to select the \"active\"\n",
    "    queries rather than the \"lazy\" queries and provides a sparse Transformer thus mitigating the quadratic compute and\n",
    "    memory requirements of vanilla attention\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        sampling_factor: int = 5,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.factor = sampling_factor\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "        # the provided `key_value_states` to support prefix tuning\n",
    "        if (\n",
    "            is_cross_attention\n",
    "            and past_key_value is not None\n",
    "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "        ):\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.reshape(*proj_shape)\n",
    "        value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "        key_states_time_length = key_states.size(1)  # L_K\n",
    "        log_key_states_time_length = np.ceil(np.log1p(key_states_time_length)).astype(\"int\").item()  # log_L_K\n",
    "\n",
    "        query_states_time_length = query_states.size(1)  # L_Q\n",
    "        log_query_states_time_length = np.ceil(np.log1p(query_states_time_length)).astype(\"int\").item()  # log_L_Q\n",
    "\n",
    "        u_part = min(self.factor * query_states_time_length * log_key_states_time_length, key_states_time_length)\n",
    "        u = min(self.factor * log_query_states_time_length, query_states_time_length)\n",
    "\n",
    "        if key_states_time_length > 0:\n",
    "            index_sample = torch.randint(0, key_states_time_length, (u_part,))\n",
    "            k_sample = key_states[:, index_sample, :]\n",
    "        else:\n",
    "            k_sample = key_states\n",
    "\n",
    "        queries_keys_sample = torch.bmm(query_states, k_sample.transpose(1, 2))  # Q_K_sampled\n",
    "\n",
    "        # find the Top_k query with sparsity measurement\n",
    "        if u > 0:\n",
    "            sparsity_measurement = queries_keys_sample.max(dim=-1)[0] - torch.div(\n",
    "                queries_keys_sample.sum(dim=-1), key_states_time_length\n",
    "            )  # M\n",
    "            top_u_sparsity_measurement = sparsity_measurement.topk(u, sorted=False)[1]  # M_top\n",
    "\n",
    "            # calculate q_reduce: query_states[:, top_u_sparsity_measurement]\n",
    "            dim_for_slice = torch.arange(query_states.size(0)).unsqueeze(-1)\n",
    "            q_reduce = query_states[dim_for_slice, top_u_sparsity_measurement]\n",
    "        else:\n",
    "            q_reduce = query_states\n",
    "            top_u_sparsity_measurement = None\n",
    "\n",
    "        # Use q_reduce to calculate attention weights\n",
    "        attn_weights = torch.bmm(q_reduce, key_states.transpose(1, 2))\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        if attn_weights.size() != (bsz * self.num_heads, u, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, u, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            prob_mask = attention_mask.expand(bsz, self.num_heads, tgt_len, src_len).reshape(\n",
    "                bsz * self.num_heads, tgt_len, src_len\n",
    "            )\n",
    "\n",
    "            if top_u_sparsity_measurement is not None:\n",
    "                dim_for_slice = torch.arange(prob_mask.size(0)).unsqueeze(-1)\n",
    "                prob_mask = prob_mask[dim_for_slice, top_u_sparsity_measurement, :]\n",
    "\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, u, src_len) + prob_mask.view(\n",
    "                bsz, self.num_heads, u, src_len\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, u, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, u, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, u, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        # calculate context for updating the attn_output, based on:\n",
    "        # https://github.com/zhouhaoyi/Informer2020/blob/ac59c7447135473fb2aafeafe94395f884d5c7a5/models/attn.py#L74\n",
    "        if self.is_decoder:\n",
    "            context = value_states.cumsum(dim=-2)\n",
    "        else:\n",
    "            v_mean_dim_time = value_states.mean(dim=-2)\n",
    "            context = (\n",
    "                v_mean_dim_time.unsqueeze(dim=1)\n",
    "                .expand(bsz * self.num_heads, query_states_time_length, v_mean_dim_time.size(-1))\n",
    "                .clone()\n",
    "            )\n",
    "\n",
    "        if top_u_sparsity_measurement is not None:\n",
    "            # update context: copy the attention output to the context at top_u_sparsity_measurement index\n",
    "            dim_for_slice = torch.arange(context.size(0)).unsqueeze(-1)\n",
    "            context[dim_for_slice, top_u_sparsity_measurement, :] = attn_output\n",
    "            attn_output = context\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned across GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value\n",
    "\n",
    "\n",
    "# source: https://github.com/zhouhaoyi/Informer2020/blob/main/models/encoder.py\n",
    "class InformerConvLayer(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        self.downConv = nn.Conv1d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=c_in,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode=\"circular\",\n",
    "        )\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x.permute(0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: InformerConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "        if config.attention_type == \"prob\":\n",
    "            self.self_attn = InformerProbSparseAttention(\n",
    "                embed_dim=self.embed_dim,\n",
    "                num_heads=config.encoder_attention_heads,\n",
    "                dropout=config.attention_dropout,\n",
    "                sampling_factor=config.sampling_factor,\n",
    "            )\n",
    "        else:\n",
    "            self.self_attn = InformerAttention(\n",
    "                embed_dim=self.embed_dim,\n",
    "                num_heads=config.encoder_attention_heads,\n",
    "                dropout=config.attention_dropout,\n",
    "            )\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        attention_mask: torch.FloatTensor,\n",
    "        layer_head_mask: torch.FloatTensor,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        hidden_states, attn_weights, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        if hidden_states.dtype == torch.float16 and (\n",
    "            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n",
    "        ):\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "class BoundEstimatorPreTrainedModel(PreTrainedModel):\n",
    "    config_class = BoundEstimatorConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    main_input_name = \"past_values\"\n",
    "    supports_gradient_checkpointing = True\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.init_std\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, InformerDecoder):\n",
    "            module.gradient_checkpointing = value\n",
    "            \n",
    "class InformerEncoder(BoundEstimatorPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Informer encoder consisting of *config.encoder_layers* self attention layers with distillation layers. Each\n",
    "    attention layer is an [`InformerEncoderLayer`].\n",
    "    Args:\n",
    "        config: InformerConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BoundEstimatorPreTrainedModel):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.encoder_layerdrop\n",
    "        self.gradient_checkpointing = False\n",
    "        if config.prediction_length is None:\n",
    "            raise ValueError(\"The `prediction_length` config needs to be specified.\")\n",
    "\n",
    "        self.value_embedding = InformerValueEmbedding(feature_size=config.feature_size, d_model=config.d_model)\n",
    "        self.embed_positions = InformerSinusoidalPositionalEmbedding(\n",
    "            config.context_length + config.prediction_length, config.d_model\n",
    "        )\n",
    "        self.layers = nn.ModuleList([InformerEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        if config.distil:\n",
    "            self.conv_layers = nn.ModuleList(\n",
    "                    [InformerConvLayer(config.d_model) for _ in range(config.encoder_layers - 1)]\n",
    "                )\n",
    "            self.conv_layers.append(None)\n",
    "        else:\n",
    "            self.conv_layers = [None] * config.encoder_layers\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        hidden_states = self.value_embedding(inputs_embeds)\n",
    "        embed_pos = self.embed_positions(inputs_embeds.size())\n",
    "\n",
    "        hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            if head_mask.size()[0] != (len(self.layers)):\n",
    "                raise ValueError(\n",
    "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "\n",
    "        for idx, (encoder_layer, conv_layer) in enumerate(zip(self.layers, self.conv_layers)):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(encoder_layer),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        (head_mask[idx] if head_mask is not None else None),\n",
    "                    )\n",
    "                    if conv_layer is not None:\n",
    "                        output = torch.utils.checkpoint.checkpoint(conv_layer, layer_outputs[0])\n",
    "                        layer_outputs = (output,) + layer_outputs[1:]\n",
    "                else:\n",
    "                    layer_outputs = encoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                        output_attentions=output_attentions,\n",
    "                    )\n",
    "                    if conv_layer is not None:\n",
    "                        output = conv_layer(layer_outputs[0])\n",
    "                        layer_outputs = (output,) + layer_outputs[1:]\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b7b8be-a555-40bf-bf94-1b3326d0a7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/torch-2.2.0.dev20231210%2Bcu121-cp311-cp311-linux_x86_64.whl (755.3 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/torchvision-0.17.0.dev20231210%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/torchaudio-2.2.0.dev20231210%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/networkx-3.0rc1-py3-none-any.whl (2.0 MB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting pytorch-triton==2.1.0+bcad9dabe1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/pytorch_triton-2.1.0%2Bbcad9dabe1-cp311-cp311-linux_x86_64.whl (183.1 MB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/numpy-1.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting requests (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/Pillow-9.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Collecting charset-normalizer<3,>=2 (from requests->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached https://download.pytorch.org/whl/nightly/mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, pytorch-triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.2.1\n",
      "    Uninstalling mpmath-1.2.1:\n",
      "      Successfully uninstalled mpmath-1.2.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.13\n",
      "    Uninstalling urllib3-1.26.13:\n",
      "      Successfully uninstalled urllib3-1.26.13\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.11.1\n",
      "    Uninstalling sympy-1.11.1:\n",
      "      Successfully uninstalled sympy-1.11.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.0rc1\n",
      "    Uninstalling networkx-3.0rc1:\n",
      "      Successfully uninstalled networkx-3.0rc1\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.3\n",
      "    Uninstalling MarkupSafe-2.1.3:\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.10.0\n",
      "    Uninstalling fsspec-2023.10.0:\n",
      "      Successfully uninstalled fsspec-2023.10.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.9.0\n",
      "    Uninstalling filelock-3.9.0:\n",
      "      Successfully uninstalled filelock-3.9.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.1.1\n",
      "    Uninstalling charset-normalizer-2.1.1:\n",
      "      Successfully uninstalled charset-normalizer-2.1.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.12.7\n",
      "    Uninstalling certifi-2022.12.7:\n",
      "      Successfully uninstalled certifi-2022.12.7\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "  Attempting uninstall: pytorch-triton\n",
      "    Found existing installation: pytorch-triton 2.1.0+bcad9dabe1\n",
      "    Uninstalling pytorch-triton-2.1.0+bcad9dabe1:\n",
      "      Successfully uninstalled pytorch-triton-2.1.0+bcad9dabe1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.2\n",
      "    Uninstalling Jinja2-3.1.2:\n",
      "      Successfully uninstalled Jinja2-3.1.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0.dev20231210+cu121\n",
      "    Uninstalling torch-2.2.0.dev20231210+cu121:\n",
      "      Successfully uninstalled torch-2.2.0.dev20231210+cu121\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.0.dev20231210+cu121\n",
      "    Uninstalling torchvision-0.17.0.dev20231210+cu121:\n",
      "      Successfully uninstalled torchvision-0.17.0.dev20231210+cu121\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.2.0.dev20231210+cu121\n",
      "    Uninstalling torchaudio-2.2.0.dev20231210+cu121:\n",
      "      Successfully uninstalled torchaudio-2.2.0.dev20231210+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.25.2 requires requests>=2.31, but you have requests 2.28.1 which is incompatible.\n",
      "huggingface-hub 0.19.4 requires fsspec>=2023.5.0, but you have fsspec 2023.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.3 certifi-2022.12.7 charset-normalizer-2.1.1 filelock-3.9.0 fsspec-2023.4.0 idna-3.4 jinja2-3.1.2 mpmath-1.2.1 networkx-3.0rc1 numpy-1.24.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 pillow-9.3.0 pytorch-triton-2.1.0+bcad9dabe1 requests-2.28.1 sympy-1.11.1 torch-2.2.0.dev20231210+cu121 torchaudio-2.2.0.dev20231210+cu121 torchvision-0.17.0.dev20231210+cu121 typing-extensions-4.8.0 urllib3-1.26.13\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --force-reinstall --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef83df87-8eb5-441c-95a8-0af228e151ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "            \n",
    "class BoundEstimatorModel(BoundEstimatorPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        if config.scaling == \"mean\" or config.scaling:\n",
    "            self.scaler = InformerMeanScaler(dim=1, keepdim=True)\n",
    "        elif config.scaling == \"std\":\n",
    "            self.scaler = InformerStdScaler(dim=1, keepdim=True)\n",
    "        else:\n",
    "            self.scaler = InformerNOPScaler(dim=1, keepdim=True)\n",
    "\n",
    "        # transformer encoder and mask initializer\n",
    "        self.encoder = InformerEncoder(config)\n",
    "        self.ff_1 = nn.ModuleList([nn.Linear(config.d_model, 32) for _ in range(config.context_length)])\n",
    "        self.ff_2 = nn.ModuleList([nn.Linear(32, 1) for _ in range(config.context_length)])\n",
    "        self.classifier2 = nn.Linear(math.ceil(config.context_length/2), config.num_labels)\n",
    "        self.classifier = nn.Linear(config.context_length, config.num_labels)\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def from_informer_model(self, model):\n",
    "        self.encoder = model.encoder\n",
    "        self.scaler = model.scaler\n",
    "        \n",
    "    @property\n",
    "    def _past_length(self):\n",
    "        return self.config.context_length + max(self.config.lags_sequence)\n",
    "    \n",
    "    def get_lagged_subsequences(\n",
    "        self, sequence: torch.Tensor, subsequences_length: int, shift: int = 0\n",
    "    ):\n",
    "        sequence_length = sequence.shape[1]\n",
    "        indices = [lag - shift for lag in self.config.lags_sequence]\n",
    "\n",
    "        if max(indices) + subsequences_length > sequence_length:\n",
    "            raise ValueError(\n",
    "                f\"lags cannot go further than history length, found lag {max(indices)} \"\n",
    "                f\"while history length is only {sequence_length}\"\n",
    "            )\n",
    "\n",
    "        lagged_values = []\n",
    "        for lag_index in indices:\n",
    "            begin_index = -lag_index - subsequences_length\n",
    "            end_index = -lag_index if lag_index > 0 else None\n",
    "            lagged_values.append(sequence[:, begin_index:end_index, ...])\n",
    "        return torch.stack(lagged_values, dim=-1)\n",
    "    \n",
    "    def create_network_inputs(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        past_observed_mask = None,\n",
    "        future_values = None,\n",
    "        future_time_features = None,\n",
    "    ):\n",
    "        # time feature\n",
    "        time_feat = (\n",
    "            torch.cat(\n",
    "                (\n",
    "                    past_time_features[:, self._past_length - self.config.context_length :, ...],\n",
    "                    future_time_features,\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            if future_values is not None\n",
    "            else past_time_features[:, self._past_length - self.config.context_length :, ...]\n",
    "        )\n",
    "\n",
    "        # target\n",
    "        if past_observed_mask is None:\n",
    "            past_observed_mask = torch.ones_like(past_values)\n",
    "\n",
    "        context = past_values[:, -self.config.context_length :]\n",
    "        observed_context = past_observed_mask[:, -self.config.context_length :]\n",
    "        _, loc, scale = self.scaler(context, observed_context)\n",
    "\n",
    "        inputs = (\n",
    "            (torch.cat((past_values, future_values), dim=1) - loc) / scale\n",
    "            if future_values is not None\n",
    "            else (past_values - loc) / scale\n",
    "        )\n",
    "\n",
    "        # static features\n",
    "        log_abs_loc = loc.abs().log1p() if self.config.input_size == 1 else loc.squeeze(1).abs().log1p()\n",
    "        log_scale = scale.log() if self.config.input_size == 1 else scale.squeeze(1).log()\n",
    "        static_feat = torch.cat((log_abs_loc, log_scale), dim=1)\n",
    "\n",
    "        expanded_static_feat = static_feat.unsqueeze(1).expand(-1, time_feat.shape[1], -1)\n",
    "\n",
    "        # all features\n",
    "        features = torch.cat((expanded_static_feat, time_feat), dim=-1)\n",
    "\n",
    "        # lagged features\n",
    "        subsequences_length = (\n",
    "            self.config.context_length + self.config.prediction_length\n",
    "            if future_values is not None\n",
    "            else self.config.context_length\n",
    "        )\n",
    "        lagged_sequence = self.get_lagged_subsequences(sequence=inputs, subsequences_length=subsequences_length)\n",
    "        lags_shape = lagged_sequence.shape\n",
    "        reshaped_lagged_sequence = lagged_sequence.reshape(lags_shape[0], lags_shape[1], -1)\n",
    "\n",
    "        if reshaped_lagged_sequence.shape[1] != time_feat.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"input length {reshaped_lagged_sequence.shape[1]} and time feature lengths {time_feat.shape[1]} does not match\"\n",
    "            )\n",
    "\n",
    "        # transformer inputs\n",
    "        transformer_inputs = torch.cat((reshaped_lagged_sequence, features), dim=-1)\n",
    "\n",
    "        return transformer_inputs, loc, scale, static_feat       \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        past_observed_mask: torch.Tensor,\n",
    "        future_values = None,\n",
    "        future_time_features = None,\n",
    "        decoder_attention_mask = None,\n",
    "        head_mask = None,\n",
    "        decoder_head_mask = None,\n",
    "        cross_attn_head_mask= None,\n",
    "        past_key_values = None,\n",
    "        output_hidden_states= None,\n",
    "        output_attentions = None,\n",
    "        use_cache = None,\n",
    "        return_dict = None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_inputs, loc, scale, static_feat = self.create_network_inputs(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            future_values=future_values,\n",
    "            future_time_features=future_time_features,\n",
    "        )\n",
    "        enc_input = transformer_inputs[:, : self.config.context_length, ...]\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=enc_input,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        ff = [torch.nn.functional.relu(self.ff_1[i](encoder_outputs.last_hidden_state[:, i, :])) for i in range(encoder_outputs.last_hidden_state.size(1))]\n",
    "        ff = [torch.nn.functional.relu(self.ff_2[i](ff[i])) for i in range(encoder_outputs.last_hidden_state.size(1))]\n",
    "        x = torch.stack(ff, dim=1).squeeze()\n",
    "        if (encoder_outputs.last_hidden_state.size(1) == self.config.context_length):\n",
    "            outputs = self.classifier(x)\n",
    "        else:\n",
    "            outputs = self.classifier2(x)\n",
    "        if not return_dict:\n",
    "            return {\"outputs\": outputs } + encoder_outputs + (loc, scale, static_feat)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae5b216a-beb7-4fa0-9fd7-37a4c01b3a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = BoundEstimatorConfig()\n",
    "config.from_informer_config(model.config)\n",
    "boundModel = BoundEstimatorModel(config)\n",
    "boundModel.from_informer_model(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e7de7d0-5479-4ad0-9a2b-7057ca988b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./resultsv3',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='steps',\n",
    "    report_to=\"none\",\n",
    "    logging_steps=250)\n",
    "\n",
    "\n",
    "class TimeSerieDataCollator:\n",
    "    def __init__(self):\n",
    "        self.default_data_collator = DefaultDataCollator()\n",
    " \n",
    "    def __call__(self, batch):\n",
    "        [x.update({'past_observed_mask': torch.ones(x[\"past_values\"].shape)}) for x in batch]\n",
    "        return self.default_data_collator(batch)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    _, results = eval_pred\n",
    "    predictions = np.argmax(results[\"actual\"], -1)\n",
    "    labels = results[\"expected\"]\n",
    "    non_zeros_labels = labels[labels != 0]\n",
    "    non_zeros_predictions = predictions[labels != 0]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', warn_for=tuple())\n",
    "    nz_precision, nz_recall, nz_f1, _ = precision_recall_fscore_support(non_zeros_labels, non_zeros_predictions, average='weighted', warn_for=tuple())\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    nz_acc = accuracy_score(non_zeros_labels, non_zeros_predictions)\n",
    "    \n",
    "    return { 'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall, \"nonzeros_pred\": np.count_nonzero(predictions) / len(predictions), \"nonzeros_labels\": np.count_nonzero(labels) / len(labels),\n",
    "            'nz_precision': nz_precision, 'nz_recall': nz_recall, 'nz_f1': nz_f1, 'nz_accuracy': nz_acc }\n",
    "\n",
    "\n",
    "class StockTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # forward pass\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        outputs = model(**inputs)\n",
    "        label = torch.zeros((inputs[\"future_values\"].size(0))).long()\n",
    "        average = torch.zeros((inputs[\"future_values\"].size(0)))\n",
    "        for i, v in enumerate(inputs[\"future_values\"]):\n",
    "            for j in v:\n",
    "                average[i] += j[features.tolist().index(\"average\") - 1].detach().cpu()\n",
    "                if average[i] >= (0.01 + daily_average):\n",
    "                    label[i] = 2\n",
    "                elif average[i] <= -(0.01 + daily_average):\n",
    "                    label[i] = 1\n",
    "        label = label.to(model.device)\n",
    "        loss = loss(outputs, label)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys= None):\n",
    "        label = torch.zeros((inputs[\"future_values\"].size(0))).long()\n",
    "        average = torch.zeros((inputs[\"future_values\"].size(0)))\n",
    "        for i, v in enumerate(inputs[\"future_values\"]):\n",
    "            for j in v:\n",
    "                average[i] += j[features.tolist().index(\"average\") - 1].detach().cpu()\n",
    "                if average[i] >= (0.01 + daily_average):\n",
    "                    label[i] = 2\n",
    "                elif average[i] <= -(0.01 + daily_average):\n",
    "                    label[i] = 1\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return (None, list(), { \"actual\": F.softmax(outputs, dim=1).detach().cpu(), \"expected\": label })\n",
    "\n",
    "\n",
    "trainer = StockTrainer(\n",
    "    model=boundModel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=TimeSerieDataCollator(),\n",
    "    compute_metrics=compute_metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908ef1d-a6f0-4484-b63e-b70b58b4dbca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63007' max='1225022' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  63007/1225022 3:53:22 < 71:44:16, 4.50 it/s, Epoch 0.10/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Nonzeros Pred</th>\n",
       "      <th>Nonzeros Labels</th>\n",
       "      <th>Nz Precision</th>\n",
       "      <th>Nz Recall</th>\n",
       "      <th>Nz F1</th>\n",
       "      <th>Nz Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.376200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.868623</td>\n",
       "      <td>0.829985</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.383700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.868623</td>\n",
       "      <td>0.829985</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.868623</td>\n",
       "      <td>0.829985</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.868623</td>\n",
       "      <td>0.829985</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.366200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.868623</td>\n",
       "      <td>0.829985</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.868623</td>\n",
       "      <td>0.829985</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.887039</td>\n",
       "      <td>0.863509</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.030362</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.250528</td>\n",
       "      <td>0.122936</td>\n",
       "      <td>0.164936</td>\n",
       "      <td>0.122936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.883739</td>\n",
       "      <td>0.859095</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.019752</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.233800</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0.122039</td>\n",
       "      <td>0.082569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.332900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.878771</td>\n",
       "      <td>0.864608</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.394233</td>\n",
       "      <td>0.047706</td>\n",
       "      <td>0.084528</td>\n",
       "      <td>0.047706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.881339</td>\n",
       "      <td>0.862163</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.015834</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.314908</td>\n",
       "      <td>0.064220</td>\n",
       "      <td>0.103670</td>\n",
       "      <td>0.064220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.331200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.882200</td>\n",
       "      <td>0.859433</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.015181</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.278175</td>\n",
       "      <td>0.071560</td>\n",
       "      <td>0.113835</td>\n",
       "      <td>0.071560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.308200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.883129</td>\n",
       "      <td>0.860288</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.015834</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.282807</td>\n",
       "      <td>0.078899</td>\n",
       "      <td>0.123378</td>\n",
       "      <td>0.078899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.891533</td>\n",
       "      <td>0.871658</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.046033</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.254523</td>\n",
       "      <td>0.177982</td>\n",
       "      <td>0.209479</td>\n",
       "      <td>0.177982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.889577</td>\n",
       "      <td>0.910517</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.028893</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.756894</td>\n",
       "      <td>0.128440</td>\n",
       "      <td>0.173854</td>\n",
       "      <td>0.128440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.884822</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.037055</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.536173</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.192520</td>\n",
       "      <td>0.146789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.888115</td>\n",
       "      <td>0.880758</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.025955</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.538523</td>\n",
       "      <td>0.111927</td>\n",
       "      <td>0.167832</td>\n",
       "      <td>0.111927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.888558</td>\n",
       "      <td>0.864844</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.028077</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.244933</td>\n",
       "      <td>0.121101</td>\n",
       "      <td>0.162070</td>\n",
       "      <td>0.121101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.890741</td>\n",
       "      <td>0.881466</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.042769</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.455317</td>\n",
       "      <td>0.159633</td>\n",
       "      <td>0.197918</td>\n",
       "      <td>0.159633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910545</td>\n",
       "      <td>0.897974</td>\n",
       "      <td>0.890641</td>\n",
       "      <td>0.910545</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.496022</td>\n",
       "      <td>0.231193</td>\n",
       "      <td>0.278289</td>\n",
       "      <td>0.231193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.353300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.891681</td>\n",
       "      <td>0.877340</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.036076</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.460568</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.196302</td>\n",
       "      <td>0.146789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.315500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.893008</td>\n",
       "      <td>0.919488</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.052236</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.753211</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.223477</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.295300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.883289</td>\n",
       "      <td>0.858369</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.019752</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.223409</td>\n",
       "      <td>0.078899</td>\n",
       "      <td>0.116615</td>\n",
       "      <td>0.078899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.307500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.887680</td>\n",
       "      <td>0.864412</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.026281</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.270092</td>\n",
       "      <td>0.117431</td>\n",
       "      <td>0.163692</td>\n",
       "      <td>0.117431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.355400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.889706</td>\n",
       "      <td>0.867395</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.035749</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.250006</td>\n",
       "      <td>0.143119</td>\n",
       "      <td>0.182032</td>\n",
       "      <td>0.143119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.885687</td>\n",
       "      <td>0.862095</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.021058</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.265746</td>\n",
       "      <td>0.097248</td>\n",
       "      <td>0.142389</td>\n",
       "      <td>0.097248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.345100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.894456</td>\n",
       "      <td>0.877183</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.055175</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.259817</td>\n",
       "      <td>0.216514</td>\n",
       "      <td>0.236197</td>\n",
       "      <td>0.216514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.294400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>0.894281</td>\n",
       "      <td>0.875777</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.254367</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>0.225076</td>\n",
       "      <td>0.201835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.343300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.888641</td>\n",
       "      <td>0.865760</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.032648</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.132110</td>\n",
       "      <td>0.173218</td>\n",
       "      <td>0.132110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.888436</td>\n",
       "      <td>0.887208</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.032484</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.498204</td>\n",
       "      <td>0.130275</td>\n",
       "      <td>0.173664</td>\n",
       "      <td>0.130275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.334700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.878456</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.056317</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>0.222018</td>\n",
       "      <td>0.238432</td>\n",
       "      <td>0.222018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.888993</td>\n",
       "      <td>0.866540</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.034280</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.274023</td>\n",
       "      <td>0.144954</td>\n",
       "      <td>0.189608</td>\n",
       "      <td>0.144954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.891353</td>\n",
       "      <td>0.881341</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.042442</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.424890</td>\n",
       "      <td>0.168807</td>\n",
       "      <td>0.207092</td>\n",
       "      <td>0.168807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.890478</td>\n",
       "      <td>0.879371</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.029873</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.495708</td>\n",
       "      <td>0.124771</td>\n",
       "      <td>0.183257</td>\n",
       "      <td>0.124771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.330900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.893925</td>\n",
       "      <td>0.883246</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.037218</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.507339</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>0.220233</td>\n",
       "      <td>0.155963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.337600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.897307</td>\n",
       "      <td>0.890765</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.062194</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.641470</td>\n",
       "      <td>0.242202</td>\n",
       "      <td>0.276347</td>\n",
       "      <td>0.242202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.893845</td>\n",
       "      <td>0.874461</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.048645</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.240996</td>\n",
       "      <td>0.190826</td>\n",
       "      <td>0.212996</td>\n",
       "      <td>0.190826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912178</td>\n",
       "      <td>0.893191</td>\n",
       "      <td>0.889638</td>\n",
       "      <td>0.912178</td>\n",
       "      <td>0.052073</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.411026</td>\n",
       "      <td>0.196330</td>\n",
       "      <td>0.219380</td>\n",
       "      <td>0.196330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.891629</td>\n",
       "      <td>0.872468</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.049135</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.233675</td>\n",
       "      <td>0.179817</td>\n",
       "      <td>0.203238</td>\n",
       "      <td>0.179817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>0.350700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.892713</td>\n",
       "      <td>0.875377</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.055011</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.232779</td>\n",
       "      <td>0.198165</td>\n",
       "      <td>0.214082</td>\n",
       "      <td>0.198165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.362300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910056</td>\n",
       "      <td>0.892245</td>\n",
       "      <td>0.875513</td>\n",
       "      <td>0.910056</td>\n",
       "      <td>0.056970</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.248954</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.228416</td>\n",
       "      <td>0.211009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>0.299600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.890223</td>\n",
       "      <td>0.868326</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.038851</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.240982</td>\n",
       "      <td>0.152294</td>\n",
       "      <td>0.186638</td>\n",
       "      <td>0.152294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.888813</td>\n",
       "      <td>0.865862</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.251826</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.177973</td>\n",
       "      <td>0.137615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.892416</td>\n",
       "      <td>0.871585</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.043258</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.242913</td>\n",
       "      <td>0.172477</td>\n",
       "      <td>0.201723</td>\n",
       "      <td>0.172477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.316800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.893824</td>\n",
       "      <td>0.920482</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.052889</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.750988</td>\n",
       "      <td>0.203670</td>\n",
       "      <td>0.224812</td>\n",
       "      <td>0.203670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.892203</td>\n",
       "      <td>0.871893</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.241752</td>\n",
       "      <td>0.176147</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.176147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.342600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.902057</td>\n",
       "      <td>0.890892</td>\n",
       "      <td>0.883030</td>\n",
       "      <td>0.902057</td>\n",
       "      <td>0.080640</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.253597</td>\n",
       "      <td>0.273394</td>\n",
       "      <td>0.263124</td>\n",
       "      <td>0.273394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.890316</td>\n",
       "      <td>0.868726</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.040157</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.241116</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>0.189409</td>\n",
       "      <td>0.155963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.895294</td>\n",
       "      <td>0.890579</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.043911</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.647343</td>\n",
       "      <td>0.187156</td>\n",
       "      <td>0.231440</td>\n",
       "      <td>0.187156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915606</td>\n",
       "      <td>0.888480</td>\n",
       "      <td>0.881597</td>\n",
       "      <td>0.915606</td>\n",
       "      <td>0.021221</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.538365</td>\n",
       "      <td>0.102752</td>\n",
       "      <td>0.170797</td>\n",
       "      <td>0.102752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.890320</td>\n",
       "      <td>0.868093</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.036892</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.254453</td>\n",
       "      <td>0.152294</td>\n",
       "      <td>0.190544</td>\n",
       "      <td>0.152294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.891541</td>\n",
       "      <td>0.872291</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.048808</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.248017</td>\n",
       "      <td>0.185321</td>\n",
       "      <td>0.212134</td>\n",
       "      <td>0.185321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909403</td>\n",
       "      <td>0.893358</td>\n",
       "      <td>0.900739</td>\n",
       "      <td>0.909403</td>\n",
       "      <td>0.062520</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.754210</td>\n",
       "      <td>0.229358</td>\n",
       "      <td>0.240880</td>\n",
       "      <td>0.229358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.899632</td>\n",
       "      <td>0.890356</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.043748</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.531282</td>\n",
       "      <td>0.188991</td>\n",
       "      <td>0.274831</td>\n",
       "      <td>0.188991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.341500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.888674</td>\n",
       "      <td>0.873928</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.031015</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.374293</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.164867</td>\n",
       "      <td>0.119266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.898790</td>\n",
       "      <td>0.892822</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.047339</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.537712</td>\n",
       "      <td>0.196330</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>0.196330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.886100</td>\n",
       "      <td>0.861811</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.027587</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.215940</td>\n",
       "      <td>0.102752</td>\n",
       "      <td>0.139246</td>\n",
       "      <td>0.102752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>0.351900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.898425</td>\n",
       "      <td>0.891071</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.056807</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.512145</td>\n",
       "      <td>0.222018</td>\n",
       "      <td>0.271053</td>\n",
       "      <td>0.222018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.897728</td>\n",
       "      <td>0.889815</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.057623</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.525177</td>\n",
       "      <td>0.220183</td>\n",
       "      <td>0.279106</td>\n",
       "      <td>0.220183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.892827</td>\n",
       "      <td>0.880954</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.531131</td>\n",
       "      <td>0.148624</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.148624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.897192</td>\n",
       "      <td>0.888809</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.043258</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.516339</td>\n",
       "      <td>0.177982</td>\n",
       "      <td>0.254132</td>\n",
       "      <td>0.177982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.892797</td>\n",
       "      <td>0.875908</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.056317</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.234024</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>0.216741</td>\n",
       "      <td>0.201835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.893005</td>\n",
       "      <td>0.872120</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.043095</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.239156</td>\n",
       "      <td>0.172477</td>\n",
       "      <td>0.200416</td>\n",
       "      <td>0.172477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.894465</td>\n",
       "      <td>0.893674</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.047176</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.534933</td>\n",
       "      <td>0.188991</td>\n",
       "      <td>0.223137</td>\n",
       "      <td>0.188991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.897454</td>\n",
       "      <td>0.888929</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.555734</td>\n",
       "      <td>0.170642</td>\n",
       "      <td>0.259965</td>\n",
       "      <td>0.170642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>0.319400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.895640</td>\n",
       "      <td>0.892186</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.050604</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.557916</td>\n",
       "      <td>0.203670</td>\n",
       "      <td>0.256336</td>\n",
       "      <td>0.203670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.902943</td>\n",
       "      <td>0.896223</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.063173</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.491107</td>\n",
       "      <td>0.236697</td>\n",
       "      <td>0.319322</td>\n",
       "      <td>0.236697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16750</td>\n",
       "      <td>0.321600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.897053</td>\n",
       "      <td>0.887606</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.039993</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.538765</td>\n",
       "      <td>0.172477</td>\n",
       "      <td>0.261250</td>\n",
       "      <td>0.172477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.898625</td>\n",
       "      <td>0.890332</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.041789</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.539921</td>\n",
       "      <td>0.183486</td>\n",
       "      <td>0.268369</td>\n",
       "      <td>0.183486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17250</td>\n",
       "      <td>0.308900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.893026</td>\n",
       "      <td>0.894788</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.044564</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.493672</td>\n",
       "      <td>0.176147</td>\n",
       "      <td>0.205755</td>\n",
       "      <td>0.176147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.344700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.891734</td>\n",
       "      <td>0.899210</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.038198</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.751751</td>\n",
       "      <td>0.157798</td>\n",
       "      <td>0.196604</td>\n",
       "      <td>0.157798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17750</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.892052</td>\n",
       "      <td>0.870933</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.041952</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.246789</td>\n",
       "      <td>0.168807</td>\n",
       "      <td>0.200482</td>\n",
       "      <td>0.168807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.894468</td>\n",
       "      <td>0.876589</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.245711</td>\n",
       "      <td>0.209174</td>\n",
       "      <td>0.225975</td>\n",
       "      <td>0.209174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18250</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911198</td>\n",
       "      <td>0.897582</td>\n",
       "      <td>0.898761</td>\n",
       "      <td>0.911198</td>\n",
       "      <td>0.064153</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.505301</td>\n",
       "      <td>0.236697</td>\n",
       "      <td>0.262434</td>\n",
       "      <td>0.236697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.898688</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.046523</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.525213</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.273091</td>\n",
       "      <td>0.192661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.896523</td>\n",
       "      <td>0.900098</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.053053</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.626606</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.237282</td>\n",
       "      <td>0.214679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912178</td>\n",
       "      <td>0.896833</td>\n",
       "      <td>0.890307</td>\n",
       "      <td>0.912178</td>\n",
       "      <td>0.054522</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.483151</td>\n",
       "      <td>0.203670</td>\n",
       "      <td>0.251981</td>\n",
       "      <td>0.203670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19250</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>0.896962</td>\n",
       "      <td>0.887394</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>0.043748</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.504698</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.256497</td>\n",
       "      <td>0.174312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.904211</td>\n",
       "      <td>0.896943</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.060562</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.521667</td>\n",
       "      <td>0.244037</td>\n",
       "      <td>0.331536</td>\n",
       "      <td>0.244037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19750</td>\n",
       "      <td>0.317600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.895010</td>\n",
       "      <td>0.921027</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.054195</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.754597</td>\n",
       "      <td>0.216514</td>\n",
       "      <td>0.239248</td>\n",
       "      <td>0.216514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.896490</td>\n",
       "      <td>0.889974</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.048972</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.516215</td>\n",
       "      <td>0.207339</td>\n",
       "      <td>0.246057</td>\n",
       "      <td>0.207339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20250</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.898034</td>\n",
       "      <td>0.889588</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.045707</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.498229</td>\n",
       "      <td>0.183486</td>\n",
       "      <td>0.259173</td>\n",
       "      <td>0.183486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.297400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.892049</td>\n",
       "      <td>0.877175</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.043258</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.423427</td>\n",
       "      <td>0.170642</td>\n",
       "      <td>0.210194</td>\n",
       "      <td>0.170642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20750</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.897359</td>\n",
       "      <td>0.887007</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.047013</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.466279</td>\n",
       "      <td>0.185321</td>\n",
       "      <td>0.247760</td>\n",
       "      <td>0.185321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.894298</td>\n",
       "      <td>0.891908</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.041952</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.540537</td>\n",
       "      <td>0.170642</td>\n",
       "      <td>0.223854</td>\n",
       "      <td>0.170642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21250</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.897208</td>\n",
       "      <td>0.881521</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.060562</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.268106</td>\n",
       "      <td>0.247706</td>\n",
       "      <td>0.257503</td>\n",
       "      <td>0.247706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.891559</td>\n",
       "      <td>0.869546</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.035913</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.251658</td>\n",
       "      <td>0.148624</td>\n",
       "      <td>0.186880</td>\n",
       "      <td>0.148624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21750</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.896328</td>\n",
       "      <td>0.896334</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.050114</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.632697</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>0.244190</td>\n",
       "      <td>0.201835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911851</td>\n",
       "      <td>0.891335</td>\n",
       "      <td>0.880615</td>\n",
       "      <td>0.911851</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.637781</td>\n",
       "      <td>0.176147</td>\n",
       "      <td>0.219243</td>\n",
       "      <td>0.176147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22250</td>\n",
       "      <td>0.314900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.893492</td>\n",
       "      <td>0.872674</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.042279</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.255876</td>\n",
       "      <td>0.176147</td>\n",
       "      <td>0.208655</td>\n",
       "      <td>0.176147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.892126</td>\n",
       "      <td>0.891357</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.035749</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.582263</td>\n",
       "      <td>0.152294</td>\n",
       "      <td>0.194565</td>\n",
       "      <td>0.152294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22750</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.899529</td>\n",
       "      <td>0.894402</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.057786</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.489188</td>\n",
       "      <td>0.225688</td>\n",
       "      <td>0.268957</td>\n",
       "      <td>0.225688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.348400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.894059</td>\n",
       "      <td>0.907862</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.049461</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.746160</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.221099</td>\n",
       "      <td>0.192661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23250</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911525</td>\n",
       "      <td>0.900127</td>\n",
       "      <td>0.899226</td>\n",
       "      <td>0.911525</td>\n",
       "      <td>0.064316</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.240367</td>\n",
       "      <td>0.289509</td>\n",
       "      <td>0.240367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>0.899146</td>\n",
       "      <td>0.890771</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>0.048482</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.508004</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.277986</td>\n",
       "      <td>0.192661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23750</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.894945</td>\n",
       "      <td>0.893400</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.051420</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.650723</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>0.240757</td>\n",
       "      <td>0.201835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.891369</td>\n",
       "      <td>0.869274</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.037708</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.251170</td>\n",
       "      <td>0.157798</td>\n",
       "      <td>0.193825</td>\n",
       "      <td>0.157798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24250</td>\n",
       "      <td>0.321100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.892645</td>\n",
       "      <td>0.872148</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.044564</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.238020</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.201244</td>\n",
       "      <td>0.174312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.290400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.898590</td>\n",
       "      <td>0.896891</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.056807</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.543591</td>\n",
       "      <td>0.220183</td>\n",
       "      <td>0.269788</td>\n",
       "      <td>0.220183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24750</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909892</td>\n",
       "      <td>0.893256</td>\n",
       "      <td>0.877894</td>\n",
       "      <td>0.909892</td>\n",
       "      <td>0.060888</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.234848</td>\n",
       "      <td>0.216514</td>\n",
       "      <td>0.225308</td>\n",
       "      <td>0.216514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912504</td>\n",
       "      <td>0.893267</td>\n",
       "      <td>0.874952</td>\n",
       "      <td>0.912504</td>\n",
       "      <td>0.052073</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.235672</td>\n",
       "      <td>0.194495</td>\n",
       "      <td>0.213113</td>\n",
       "      <td>0.194495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25250</td>\n",
       "      <td>0.318700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.903095</td>\n",
       "      <td>0.899817</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.068723</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.526502</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.340766</td>\n",
       "      <td>0.264220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.321200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.898139</td>\n",
       "      <td>0.894329</td>\n",
       "      <td>0.901933</td>\n",
       "      <td>0.898139</td>\n",
       "      <td>0.090108</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.511763</td>\n",
       "      <td>0.289908</td>\n",
       "      <td>0.315913</td>\n",
       "      <td>0.289908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25750</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.896856</td>\n",
       "      <td>0.884623</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.053542</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.440787</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.245317</td>\n",
       "      <td>0.214679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.898076</td>\n",
       "      <td>0.888378</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.054358</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.506299</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.265239</td>\n",
       "      <td>0.214679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26250</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.897551</td>\n",
       "      <td>0.887065</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.041463</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.510781</td>\n",
       "      <td>0.179817</td>\n",
       "      <td>0.249610</td>\n",
       "      <td>0.179817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.896924</td>\n",
       "      <td>0.887483</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.046360</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.458856</td>\n",
       "      <td>0.181651</td>\n",
       "      <td>0.238617</td>\n",
       "      <td>0.181651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26750</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910382</td>\n",
       "      <td>0.902752</td>\n",
       "      <td>0.896343</td>\n",
       "      <td>0.910382</td>\n",
       "      <td>0.068723</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.478406</td>\n",
       "      <td>0.251376</td>\n",
       "      <td>0.319783</td>\n",
       "      <td>0.251376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.334500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.892488</td>\n",
       "      <td>0.875159</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.037708</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.384187</td>\n",
       "      <td>0.159633</td>\n",
       "      <td>0.200240</td>\n",
       "      <td>0.159633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27250</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.898267</td>\n",
       "      <td>0.888318</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.050114</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.471174</td>\n",
       "      <td>0.198165</td>\n",
       "      <td>0.256610</td>\n",
       "      <td>0.198165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.340800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.902357</td>\n",
       "      <td>0.895430</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>0.062684</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.507585</td>\n",
       "      <td>0.251376</td>\n",
       "      <td>0.306057</td>\n",
       "      <td>0.251376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27750</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.896252</td>\n",
       "      <td>0.883313</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.055501</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.360658</td>\n",
       "      <td>0.222018</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.222018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.357200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.896917</td>\n",
       "      <td>0.878959</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.053216</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.262992</td>\n",
       "      <td>0.222018</td>\n",
       "      <td>0.240774</td>\n",
       "      <td>0.222018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28250</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.890751</td>\n",
       "      <td>0.886578</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.029056</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.613558</td>\n",
       "      <td>0.132110</td>\n",
       "      <td>0.185027</td>\n",
       "      <td>0.132110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911361</td>\n",
       "      <td>0.896111</td>\n",
       "      <td>0.885925</td>\n",
       "      <td>0.911361</td>\n",
       "      <td>0.062847</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.503003</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.252306</td>\n",
       "      <td>0.238532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28750</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910219</td>\n",
       "      <td>0.897514</td>\n",
       "      <td>0.895323</td>\n",
       "      <td>0.910219</td>\n",
       "      <td>0.063826</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.242202</td>\n",
       "      <td>0.281872</td>\n",
       "      <td>0.242202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.900513</td>\n",
       "      <td>0.892624</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.060235</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.442817</td>\n",
       "      <td>0.207339</td>\n",
       "      <td>0.282231</td>\n",
       "      <td>0.207339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29250</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910872</td>\n",
       "      <td>0.900377</td>\n",
       "      <td>0.893016</td>\n",
       "      <td>0.910872</td>\n",
       "      <td>0.067091</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.479769</td>\n",
       "      <td>0.253211</td>\n",
       "      <td>0.297045</td>\n",
       "      <td>0.253211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.305100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.891958</td>\n",
       "      <td>0.870157</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.255299</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>0.200551</td>\n",
       "      <td>0.165138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29750</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.901275</td>\n",
       "      <td>0.895934</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.062847</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.461105</td>\n",
       "      <td>0.220183</td>\n",
       "      <td>0.293825</td>\n",
       "      <td>0.220183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910382</td>\n",
       "      <td>0.901021</td>\n",
       "      <td>0.894349</td>\n",
       "      <td>0.910382</td>\n",
       "      <td>0.062357</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.487481</td>\n",
       "      <td>0.227523</td>\n",
       "      <td>0.309092</td>\n",
       "      <td>0.227523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30250</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.889654</td>\n",
       "      <td>0.884543</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.031832</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.472266</td>\n",
       "      <td>0.130275</td>\n",
       "      <td>0.184225</td>\n",
       "      <td>0.130275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.345800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.893251</td>\n",
       "      <td>0.875793</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.054685</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.230764</td>\n",
       "      <td>0.198165</td>\n",
       "      <td>0.213226</td>\n",
       "      <td>0.198165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30750</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.917075</td>\n",
       "      <td>0.899645</td>\n",
       "      <td>0.891094</td>\n",
       "      <td>0.917075</td>\n",
       "      <td>0.039830</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.553115</td>\n",
       "      <td>0.183486</td>\n",
       "      <td>0.275220</td>\n",
       "      <td>0.183486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.351200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.899192</td>\n",
       "      <td>0.889990</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.050278</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.484515</td>\n",
       "      <td>0.198165</td>\n",
       "      <td>0.266408</td>\n",
       "      <td>0.198165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31250</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.899220</td>\n",
       "      <td>0.888870</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.049625</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.473057</td>\n",
       "      <td>0.194495</td>\n",
       "      <td>0.266172</td>\n",
       "      <td>0.194495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.321200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.892145</td>\n",
       "      <td>0.892085</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.037871</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.157798</td>\n",
       "      <td>0.196985</td>\n",
       "      <td>0.157798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31750</td>\n",
       "      <td>0.311600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.892680</td>\n",
       "      <td>0.871934</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.043585</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.246789</td>\n",
       "      <td>0.176147</td>\n",
       "      <td>0.205568</td>\n",
       "      <td>0.176147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.321600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.894058</td>\n",
       "      <td>0.888890</td>\n",
       "      <td>0.895670</td>\n",
       "      <td>0.894058</td>\n",
       "      <td>0.079987</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.490416</td>\n",
       "      <td>0.231193</td>\n",
       "      <td>0.288613</td>\n",
       "      <td>0.231193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32250</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912178</td>\n",
       "      <td>0.893758</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.912178</td>\n",
       "      <td>0.055011</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.241447</td>\n",
       "      <td>0.207339</td>\n",
       "      <td>0.223097</td>\n",
       "      <td>0.207339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.322600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.892480</td>\n",
       "      <td>0.888701</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.043258</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.463639</td>\n",
       "      <td>0.166972</td>\n",
       "      <td>0.205491</td>\n",
       "      <td>0.166972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32750</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.893007</td>\n",
       "      <td>0.874591</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.051747</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.238901</td>\n",
       "      <td>0.194495</td>\n",
       "      <td>0.214423</td>\n",
       "      <td>0.194495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.900907</td>\n",
       "      <td>0.893184</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.050278</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.512444</td>\n",
       "      <td>0.203670</td>\n",
       "      <td>0.290717</td>\n",
       "      <td>0.203670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33250</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.899911</td>\n",
       "      <td>0.891244</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.057460</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.448940</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.274621</td>\n",
       "      <td>0.214679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.896453</td>\n",
       "      <td>0.884698</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.062684</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.351927</td>\n",
       "      <td>0.236697</td>\n",
       "      <td>0.247295</td>\n",
       "      <td>0.236697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33750</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.900265</td>\n",
       "      <td>0.891910</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.481667</td>\n",
       "      <td>0.231193</td>\n",
       "      <td>0.287753</td>\n",
       "      <td>0.231193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.900695</td>\n",
       "      <td>0.891903</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.051420</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.492522</td>\n",
       "      <td>0.205505</td>\n",
       "      <td>0.280138</td>\n",
       "      <td>0.205505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34250</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.895740</td>\n",
       "      <td>0.884712</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.043095</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.459197</td>\n",
       "      <td>0.159633</td>\n",
       "      <td>0.235211</td>\n",
       "      <td>0.159633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.316400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.895111</td>\n",
       "      <td>0.884398</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.046360</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.458252</td>\n",
       "      <td>0.172477</td>\n",
       "      <td>0.235320</td>\n",
       "      <td>0.172477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34750</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.898649</td>\n",
       "      <td>0.890912</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.047013</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.501535</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.260011</td>\n",
       "      <td>0.192661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.894141</td>\n",
       "      <td>0.883181</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.032811</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.488648</td>\n",
       "      <td>0.143119</td>\n",
       "      <td>0.213326</td>\n",
       "      <td>0.143119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35250</td>\n",
       "      <td>0.341100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.894307</td>\n",
       "      <td>0.887081</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.035260</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.486173</td>\n",
       "      <td>0.144954</td>\n",
       "      <td>0.216553</td>\n",
       "      <td>0.144954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.900037</td>\n",
       "      <td>0.891963</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.046849</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.529675</td>\n",
       "      <td>0.198165</td>\n",
       "      <td>0.285682</td>\n",
       "      <td>0.198165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35750</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915606</td>\n",
       "      <td>0.893610</td>\n",
       "      <td>0.885576</td>\n",
       "      <td>0.915606</td>\n",
       "      <td>0.034770</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.525044</td>\n",
       "      <td>0.150459</td>\n",
       "      <td>0.213935</td>\n",
       "      <td>0.150459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.908097</td>\n",
       "      <td>0.897917</td>\n",
       "      <td>0.889481</td>\n",
       "      <td>0.908097</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.465303</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.285293</td>\n",
       "      <td>0.214679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36250</td>\n",
       "      <td>0.357300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.897865</td>\n",
       "      <td>0.888926</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.041463</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.507601</td>\n",
       "      <td>0.172477</td>\n",
       "      <td>0.255792</td>\n",
       "      <td>0.172477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.898065</td>\n",
       "      <td>0.888385</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.042769</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.506182</td>\n",
       "      <td>0.176147</td>\n",
       "      <td>0.260867</td>\n",
       "      <td>0.176147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36750</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.895768</td>\n",
       "      <td>0.885746</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.035423</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.511832</td>\n",
       "      <td>0.154128</td>\n",
       "      <td>0.233139</td>\n",
       "      <td>0.154128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.337600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911851</td>\n",
       "      <td>0.900107</td>\n",
       "      <td>0.891029</td>\n",
       "      <td>0.911851</td>\n",
       "      <td>0.058276</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.452123</td>\n",
       "      <td>0.212844</td>\n",
       "      <td>0.280954</td>\n",
       "      <td>0.212844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37250</td>\n",
       "      <td>0.289800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910382</td>\n",
       "      <td>0.895815</td>\n",
       "      <td>0.884521</td>\n",
       "      <td>0.910382</td>\n",
       "      <td>0.051747</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.416916</td>\n",
       "      <td>0.176147</td>\n",
       "      <td>0.239511</td>\n",
       "      <td>0.176147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.899136</td>\n",
       "      <td>0.890241</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.056154</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.438133</td>\n",
       "      <td>0.209174</td>\n",
       "      <td>0.264395</td>\n",
       "      <td>0.209174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37750</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.897353</td>\n",
       "      <td>0.886583</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.058439</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.382883</td>\n",
       "      <td>0.231193</td>\n",
       "      <td>0.249584</td>\n",
       "      <td>0.231193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.307500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.892755</td>\n",
       "      <td>0.884505</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.037708</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.455703</td>\n",
       "      <td>0.154128</td>\n",
       "      <td>0.201085</td>\n",
       "      <td>0.154128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38250</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.899126</td>\n",
       "      <td>0.895683</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.061704</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.470385</td>\n",
       "      <td>0.236697</td>\n",
       "      <td>0.270363</td>\n",
       "      <td>0.236697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.899207</td>\n",
       "      <td>0.894596</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.058113</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.469981</td>\n",
       "      <td>0.231193</td>\n",
       "      <td>0.270230</td>\n",
       "      <td>0.231193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38750</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.892678</td>\n",
       "      <td>0.884249</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.035913</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.479689</td>\n",
       "      <td>0.143119</td>\n",
       "      <td>0.203148</td>\n",
       "      <td>0.143119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.327100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.896236</td>\n",
       "      <td>0.886860</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.043258</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.471155</td>\n",
       "      <td>0.176147</td>\n",
       "      <td>0.237741</td>\n",
       "      <td>0.176147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39250</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.898890</td>\n",
       "      <td>0.895873</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.063337</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.480287</td>\n",
       "      <td>0.236697</td>\n",
       "      <td>0.275056</td>\n",
       "      <td>0.236697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.374500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.900516</td>\n",
       "      <td>0.895714</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.060725</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.483547</td>\n",
       "      <td>0.233028</td>\n",
       "      <td>0.286023</td>\n",
       "      <td>0.233028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39750</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.896533</td>\n",
       "      <td>0.888459</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.050767</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.440563</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.244483</td>\n",
       "      <td>0.192661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.354400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.903526</td>\n",
       "      <td>0.895915</td>\n",
       "      <td>0.894943</td>\n",
       "      <td>0.903526</td>\n",
       "      <td>0.078844</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.451077</td>\n",
       "      <td>0.258716</td>\n",
       "      <td>0.286643</td>\n",
       "      <td>0.258716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40250</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.899430</td>\n",
       "      <td>0.890894</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.054848</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.476785</td>\n",
       "      <td>0.209174</td>\n",
       "      <td>0.277265</td>\n",
       "      <td>0.209174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.899160</td>\n",
       "      <td>0.890234</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.045870</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.493963</td>\n",
       "      <td>0.187156</td>\n",
       "      <td>0.267233</td>\n",
       "      <td>0.187156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40750</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.898605</td>\n",
       "      <td>0.889060</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.050930</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.458236</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.262675</td>\n",
       "      <td>0.192661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.903435</td>\n",
       "      <td>0.898189</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.058439</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.537055</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.321325</td>\n",
       "      <td>0.238532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41250</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.899075</td>\n",
       "      <td>0.889598</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.049951</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.490478</td>\n",
       "      <td>0.194495</td>\n",
       "      <td>0.275572</td>\n",
       "      <td>0.194495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.297300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.897666</td>\n",
       "      <td>0.887384</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.047992</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.461031</td>\n",
       "      <td>0.177982</td>\n",
       "      <td>0.255672</td>\n",
       "      <td>0.177982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41750</td>\n",
       "      <td>0.299100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.902952</td>\n",
       "      <td>0.895570</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.060888</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.485145</td>\n",
       "      <td>0.229358</td>\n",
       "      <td>0.311227</td>\n",
       "      <td>0.229358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.917238</td>\n",
       "      <td>0.901906</td>\n",
       "      <td>0.894564</td>\n",
       "      <td>0.917238</td>\n",
       "      <td>0.045544</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.551597</td>\n",
       "      <td>0.205505</td>\n",
       "      <td>0.295331</td>\n",
       "      <td>0.205505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42250</td>\n",
       "      <td>0.288300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.901638</td>\n",
       "      <td>0.893106</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.056317</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.474186</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.214679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.899978</td>\n",
       "      <td>0.890825</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.053542</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.475023</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>0.281331</td>\n",
       "      <td>0.201835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42750</td>\n",
       "      <td>0.304600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.917238</td>\n",
       "      <td>0.897550</td>\n",
       "      <td>0.888964</td>\n",
       "      <td>0.917238</td>\n",
       "      <td>0.035096</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.533634</td>\n",
       "      <td>0.161468</td>\n",
       "      <td>0.247877</td>\n",
       "      <td>0.161468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909729</td>\n",
       "      <td>0.901793</td>\n",
       "      <td>0.895187</td>\n",
       "      <td>0.909729</td>\n",
       "      <td>0.067091</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.468852</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.312445</td>\n",
       "      <td>0.238532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43250</td>\n",
       "      <td>0.305800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916585</td>\n",
       "      <td>0.902421</td>\n",
       "      <td>0.894485</td>\n",
       "      <td>0.916585</td>\n",
       "      <td>0.048645</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.525655</td>\n",
       "      <td>0.209174</td>\n",
       "      <td>0.296895</td>\n",
       "      <td>0.209174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.903246</td>\n",
       "      <td>0.896397</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.064479</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.486969</td>\n",
       "      <td>0.240367</td>\n",
       "      <td>0.321269</td>\n",
       "      <td>0.240367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43750</td>\n",
       "      <td>0.338800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.903550</td>\n",
       "      <td>0.896372</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.061378</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.481488</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.309724</td>\n",
       "      <td>0.238532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.312900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.901713</td>\n",
       "      <td>0.893683</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.050278</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.535568</td>\n",
       "      <td>0.212844</td>\n",
       "      <td>0.303999</td>\n",
       "      <td>0.212844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44250</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>0.900471</td>\n",
       "      <td>0.891319</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>0.052236</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.483748</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>0.283572</td>\n",
       "      <td>0.201835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.302400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916748</td>\n",
       "      <td>0.903303</td>\n",
       "      <td>0.896064</td>\n",
       "      <td>0.916748</td>\n",
       "      <td>0.050767</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.537735</td>\n",
       "      <td>0.220183</td>\n",
       "      <td>0.307017</td>\n",
       "      <td>0.220183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44750</td>\n",
       "      <td>0.300200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.903339</td>\n",
       "      <td>0.899509</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.059582</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.542936</td>\n",
       "      <td>0.245872</td>\n",
       "      <td>0.319426</td>\n",
       "      <td>0.245872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911851</td>\n",
       "      <td>0.901420</td>\n",
       "      <td>0.893604</td>\n",
       "      <td>0.911851</td>\n",
       "      <td>0.059419</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.465633</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.293499</td>\n",
       "      <td>0.214679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45250</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.893516</td>\n",
       "      <td>0.883982</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.464527</td>\n",
       "      <td>0.141284</td>\n",
       "      <td>0.210991</td>\n",
       "      <td>0.141284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.311700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.900917</td>\n",
       "      <td>0.896004</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.054195</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.528547</td>\n",
       "      <td>0.223853</td>\n",
       "      <td>0.287707</td>\n",
       "      <td>0.223853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45750</td>\n",
       "      <td>0.358800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.890533</td>\n",
       "      <td>0.880003</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.027587</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.457517</td>\n",
       "      <td>0.110092</td>\n",
       "      <td>0.171978</td>\n",
       "      <td>0.110092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.314600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.899950</td>\n",
       "      <td>0.891852</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.053379</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.486608</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>0.283997</td>\n",
       "      <td>0.201835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46250</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.898276</td>\n",
       "      <td>0.887991</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.045380</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.460537</td>\n",
       "      <td>0.172477</td>\n",
       "      <td>0.250900</td>\n",
       "      <td>0.172477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.890290</td>\n",
       "      <td>0.880962</td>\n",
       "      <td>0.914953</td>\n",
       "      <td>0.028893</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.466650</td>\n",
       "      <td>0.113761</td>\n",
       "      <td>0.171338</td>\n",
       "      <td>0.113761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46750</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.897643</td>\n",
       "      <td>0.889498</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.043258</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.254340</td>\n",
       "      <td>0.174312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.891963</td>\n",
       "      <td>0.869516</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.036076</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.242408</td>\n",
       "      <td>0.152294</td>\n",
       "      <td>0.187064</td>\n",
       "      <td>0.152294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47250</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.900460</td>\n",
       "      <td>0.894390</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.048645</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.535312</td>\n",
       "      <td>0.205505</td>\n",
       "      <td>0.281245</td>\n",
       "      <td>0.205505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.334300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910382</td>\n",
       "      <td>0.897711</td>\n",
       "      <td>0.888489</td>\n",
       "      <td>0.910382</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.480026</td>\n",
       "      <td>0.194495</td>\n",
       "      <td>0.275294</td>\n",
       "      <td>0.194495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47750</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.901340</td>\n",
       "      <td>0.893266</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.059582</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.457687</td>\n",
       "      <td>0.218349</td>\n",
       "      <td>0.290213</td>\n",
       "      <td>0.218349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.917238</td>\n",
       "      <td>0.895054</td>\n",
       "      <td>0.886869</td>\n",
       "      <td>0.917238</td>\n",
       "      <td>0.030199</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.531803</td>\n",
       "      <td>0.141284</td>\n",
       "      <td>0.221470</td>\n",
       "      <td>0.141284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48250</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.899198</td>\n",
       "      <td>0.891669</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.044564</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.525993</td>\n",
       "      <td>0.187156</td>\n",
       "      <td>0.270225</td>\n",
       "      <td>0.187156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.285200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.901735</td>\n",
       "      <td>0.894255</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.062684</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.468791</td>\n",
       "      <td>0.225688</td>\n",
       "      <td>0.303451</td>\n",
       "      <td>0.225688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48750</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.895926</td>\n",
       "      <td>0.885382</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.043748</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.476496</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>0.244866</td>\n",
       "      <td>0.165138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.900913</td>\n",
       "      <td>0.892586</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.054032</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.496474</td>\n",
       "      <td>0.209174</td>\n",
       "      <td>0.294199</td>\n",
       "      <td>0.209174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49250</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911525</td>\n",
       "      <td>0.898592</td>\n",
       "      <td>0.889122</td>\n",
       "      <td>0.911525</td>\n",
       "      <td>0.053869</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.461562</td>\n",
       "      <td>0.194495</td>\n",
       "      <td>0.270932</td>\n",
       "      <td>0.194495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.905648</td>\n",
       "      <td>0.903434</td>\n",
       "      <td>0.903246</td>\n",
       "      <td>0.905648</td>\n",
       "      <td>0.083088</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.513926</td>\n",
       "      <td>0.295413</td>\n",
       "      <td>0.369628</td>\n",
       "      <td>0.295413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49750</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916422</td>\n",
       "      <td>0.898865</td>\n",
       "      <td>0.892245</td>\n",
       "      <td>0.916422</td>\n",
       "      <td>0.047176</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.516881</td>\n",
       "      <td>0.198165</td>\n",
       "      <td>0.253441</td>\n",
       "      <td>0.198165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.341800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.893540</td>\n",
       "      <td>0.889455</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.038198</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.617792</td>\n",
       "      <td>0.159633</td>\n",
       "      <td>0.204372</td>\n",
       "      <td>0.159633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50250</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.893145</td>\n",
       "      <td>0.871974</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.041952</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.240329</td>\n",
       "      <td>0.170642</td>\n",
       "      <td>0.199577</td>\n",
       "      <td>0.170642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.352500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.894108</td>\n",
       "      <td>0.885908</td>\n",
       "      <td>0.915769</td>\n",
       "      <td>0.039177</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.527685</td>\n",
       "      <td>0.159633</td>\n",
       "      <td>0.204509</td>\n",
       "      <td>0.159633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50750</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.892121</td>\n",
       "      <td>0.870303</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.039177</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.239895</td>\n",
       "      <td>0.159633</td>\n",
       "      <td>0.191702</td>\n",
       "      <td>0.159633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.303300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.895490</td>\n",
       "      <td>0.897729</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.056481</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.484437</td>\n",
       "      <td>0.205505</td>\n",
       "      <td>0.236841</td>\n",
       "      <td>0.205505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51250</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916259</td>\n",
       "      <td>0.889723</td>\n",
       "      <td>0.895597</td>\n",
       "      <td>0.916259</td>\n",
       "      <td>0.027914</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.584404</td>\n",
       "      <td>0.124771</td>\n",
       "      <td>0.169735</td>\n",
       "      <td>0.124771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.893254</td>\n",
       "      <td>0.890588</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.044564</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.446296</td>\n",
       "      <td>0.166972</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>0.166972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51750</td>\n",
       "      <td>0.291700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.894825</td>\n",
       "      <td>0.889599</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.045870</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.599750</td>\n",
       "      <td>0.183486</td>\n",
       "      <td>0.219139</td>\n",
       "      <td>0.183486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.899032</td>\n",
       "      <td>0.893169</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.504097</td>\n",
       "      <td>0.205505</td>\n",
       "      <td>0.265333</td>\n",
       "      <td>0.205505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52250</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916259</td>\n",
       "      <td>0.892323</td>\n",
       "      <td>0.883122</td>\n",
       "      <td>0.916259</td>\n",
       "      <td>0.031015</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.482782</td>\n",
       "      <td>0.130275</td>\n",
       "      <td>0.186204</td>\n",
       "      <td>0.130275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910382</td>\n",
       "      <td>0.892831</td>\n",
       "      <td>0.920877</td>\n",
       "      <td>0.910382</td>\n",
       "      <td>0.056807</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.734227</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.215610</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52750</td>\n",
       "      <td>0.311900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.902658</td>\n",
       "      <td>0.894896</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.057623</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.502453</td>\n",
       "      <td>0.225688</td>\n",
       "      <td>0.311182</td>\n",
       "      <td>0.225688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916422</td>\n",
       "      <td>0.900288</td>\n",
       "      <td>0.892051</td>\n",
       "      <td>0.916422</td>\n",
       "      <td>0.044074</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.525597</td>\n",
       "      <td>0.190826</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.190826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53250</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.902827</td>\n",
       "      <td>0.894977</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.051257</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.524898</td>\n",
       "      <td>0.216514</td>\n",
       "      <td>0.304869</td>\n",
       "      <td>0.216514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.315200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.902119</td>\n",
       "      <td>0.896119</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.060888</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.535745</td>\n",
       "      <td>0.251376</td>\n",
       "      <td>0.310948</td>\n",
       "      <td>0.251376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53750</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911361</td>\n",
       "      <td>0.896157</td>\n",
       "      <td>0.887164</td>\n",
       "      <td>0.911361</td>\n",
       "      <td>0.047339</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.536530</td>\n",
       "      <td>0.187156</td>\n",
       "      <td>0.274436</td>\n",
       "      <td>0.187156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.900782</td>\n",
       "      <td>0.892261</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.053216</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.497741</td>\n",
       "      <td>0.222018</td>\n",
       "      <td>0.276781</td>\n",
       "      <td>0.222018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54250</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915606</td>\n",
       "      <td>0.900109</td>\n",
       "      <td>0.891895</td>\n",
       "      <td>0.915606</td>\n",
       "      <td>0.047502</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.545626</td>\n",
       "      <td>0.205505</td>\n",
       "      <td>0.285284</td>\n",
       "      <td>0.205505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.897114</td>\n",
       "      <td>0.886833</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.046523</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.471769</td>\n",
       "      <td>0.188991</td>\n",
       "      <td>0.239945</td>\n",
       "      <td>0.188991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54750</td>\n",
       "      <td>0.338800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916748</td>\n",
       "      <td>0.891117</td>\n",
       "      <td>0.888182</td>\n",
       "      <td>0.916748</td>\n",
       "      <td>0.027098</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.527274</td>\n",
       "      <td>0.126606</td>\n",
       "      <td>0.185017</td>\n",
       "      <td>0.126606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.301200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.896827</td>\n",
       "      <td>0.889185</td>\n",
       "      <td>0.913973</td>\n",
       "      <td>0.054358</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.536546</td>\n",
       "      <td>0.220183</td>\n",
       "      <td>0.251644</td>\n",
       "      <td>0.220183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55250</td>\n",
       "      <td>0.311200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912178</td>\n",
       "      <td>0.899329</td>\n",
       "      <td>0.894080</td>\n",
       "      <td>0.912178</td>\n",
       "      <td>0.067254</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.542987</td>\n",
       "      <td>0.262385</td>\n",
       "      <td>0.282058</td>\n",
       "      <td>0.262385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915606</td>\n",
       "      <td>0.897775</td>\n",
       "      <td>0.887988</td>\n",
       "      <td>0.915606</td>\n",
       "      <td>0.047502</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.505606</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>0.254225</td>\n",
       "      <td>0.201835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55750</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.897057</td>\n",
       "      <td>0.886565</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.041789</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.506330</td>\n",
       "      <td>0.177982</td>\n",
       "      <td>0.240012</td>\n",
       "      <td>0.177982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.897697</td>\n",
       "      <td>0.890339</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.053542</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.512058</td>\n",
       "      <td>0.225688</td>\n",
       "      <td>0.248956</td>\n",
       "      <td>0.225688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56250</td>\n",
       "      <td>0.331400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916422</td>\n",
       "      <td>0.895240</td>\n",
       "      <td>0.883039</td>\n",
       "      <td>0.916422</td>\n",
       "      <td>0.042932</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.425733</td>\n",
       "      <td>0.183486</td>\n",
       "      <td>0.220181</td>\n",
       "      <td>0.183486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.917728</td>\n",
       "      <td>0.891246</td>\n",
       "      <td>0.889334</td>\n",
       "      <td>0.917728</td>\n",
       "      <td>0.026118</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.551592</td>\n",
       "      <td>0.128440</td>\n",
       "      <td>0.181426</td>\n",
       "      <td>0.128440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56750</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.897226</td>\n",
       "      <td>0.880159</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.056154</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.261513</td>\n",
       "      <td>0.231193</td>\n",
       "      <td>0.245420</td>\n",
       "      <td>0.231193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.896085</td>\n",
       "      <td>0.877600</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.051420</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.263339</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.236532</td>\n",
       "      <td>0.214679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57250</td>\n",
       "      <td>0.346800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.895323</td>\n",
       "      <td>0.878022</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.054685</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.255394</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.233273</td>\n",
       "      <td>0.214679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.892468</td>\n",
       "      <td>0.871865</td>\n",
       "      <td>0.914136</td>\n",
       "      <td>0.043258</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.243931</td>\n",
       "      <td>0.168807</td>\n",
       "      <td>0.199532</td>\n",
       "      <td>0.168807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57750</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>0.895976</td>\n",
       "      <td>0.883580</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.378770</td>\n",
       "      <td>0.218349</td>\n",
       "      <td>0.237960</td>\n",
       "      <td>0.218349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916585</td>\n",
       "      <td>0.897238</td>\n",
       "      <td>0.890336</td>\n",
       "      <td>0.916585</td>\n",
       "      <td>0.041626</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.617020</td>\n",
       "      <td>0.188991</td>\n",
       "      <td>0.251764</td>\n",
       "      <td>0.188991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58250</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909729</td>\n",
       "      <td>0.895716</td>\n",
       "      <td>0.885600</td>\n",
       "      <td>0.909729</td>\n",
       "      <td>0.065295</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.396975</td>\n",
       "      <td>0.242202</td>\n",
       "      <td>0.259447</td>\n",
       "      <td>0.242202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.917564</td>\n",
       "      <td>0.893086</td>\n",
       "      <td>0.896650</td>\n",
       "      <td>0.917564</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.627165</td>\n",
       "      <td>0.150459</td>\n",
       "      <td>0.204460</td>\n",
       "      <td>0.150459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58750</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.907607</td>\n",
       "      <td>0.895900</td>\n",
       "      <td>0.895623</td>\n",
       "      <td>0.907607</td>\n",
       "      <td>0.077049</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.590876</td>\n",
       "      <td>0.282569</td>\n",
       "      <td>0.277350</td>\n",
       "      <td>0.282569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.297100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.898183</td>\n",
       "      <td>0.888559</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.060235</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.433571</td>\n",
       "      <td>0.233028</td>\n",
       "      <td>0.262695</td>\n",
       "      <td>0.233028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59250</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.897060</td>\n",
       "      <td>0.887081</td>\n",
       "      <td>0.915279</td>\n",
       "      <td>0.044564</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.540085</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.254435</td>\n",
       "      <td>0.192661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.327900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916912</td>\n",
       "      <td>0.896861</td>\n",
       "      <td>0.888493</td>\n",
       "      <td>0.916912</td>\n",
       "      <td>0.034117</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.552078</td>\n",
       "      <td>0.159633</td>\n",
       "      <td>0.247546</td>\n",
       "      <td>0.159633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59750</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916748</td>\n",
       "      <td>0.894042</td>\n",
       "      <td>0.885448</td>\n",
       "      <td>0.916748</td>\n",
       "      <td>0.029220</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.518275</td>\n",
       "      <td>0.133945</td>\n",
       "      <td>0.212793</td>\n",
       "      <td>0.133945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.917075</td>\n",
       "      <td>0.897061</td>\n",
       "      <td>0.888451</td>\n",
       "      <td>0.917075</td>\n",
       "      <td>0.035913</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.506547</td>\n",
       "      <td>0.157798</td>\n",
       "      <td>0.235688</td>\n",
       "      <td>0.157798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60250</td>\n",
       "      <td>0.326900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.890830</td>\n",
       "      <td>0.890265</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.037708</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.509789</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.200406</td>\n",
       "      <td>0.146789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916259</td>\n",
       "      <td>0.893284</td>\n",
       "      <td>0.888510</td>\n",
       "      <td>0.916259</td>\n",
       "      <td>0.030689</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.536173</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.208389</td>\n",
       "      <td>0.137615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60750</td>\n",
       "      <td>0.335500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911851</td>\n",
       "      <td>0.896364</td>\n",
       "      <td>0.887433</td>\n",
       "      <td>0.911851</td>\n",
       "      <td>0.047992</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.460339</td>\n",
       "      <td>0.172477</td>\n",
       "      <td>0.246966</td>\n",
       "      <td>0.172477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.287000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.897014</td>\n",
       "      <td>0.886339</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.046197</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.449850</td>\n",
       "      <td>0.168807</td>\n",
       "      <td>0.244266</td>\n",
       "      <td>0.168807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61250</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.897941</td>\n",
       "      <td>0.889234</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.060562</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.492966</td>\n",
       "      <td>0.222018</td>\n",
       "      <td>0.268629</td>\n",
       "      <td>0.222018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.332300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.898126</td>\n",
       "      <td>0.889184</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.467501</td>\n",
       "      <td>0.218349</td>\n",
       "      <td>0.260395</td>\n",
       "      <td>0.218349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61750</td>\n",
       "      <td>0.305700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.899082</td>\n",
       "      <td>0.889734</td>\n",
       "      <td>0.914789</td>\n",
       "      <td>0.046197</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.503272</td>\n",
       "      <td>0.187156</td>\n",
       "      <td>0.267874</td>\n",
       "      <td>0.187156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.296600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916259</td>\n",
       "      <td>0.900107</td>\n",
       "      <td>0.893407</td>\n",
       "      <td>0.916259</td>\n",
       "      <td>0.045870</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.532752</td>\n",
       "      <td>0.196330</td>\n",
       "      <td>0.274452</td>\n",
       "      <td>0.196330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62250</td>\n",
       "      <td>0.344500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912504</td>\n",
       "      <td>0.901303</td>\n",
       "      <td>0.892916</td>\n",
       "      <td>0.912504</td>\n",
       "      <td>0.057297</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.485050</td>\n",
       "      <td>0.216514</td>\n",
       "      <td>0.298216</td>\n",
       "      <td>0.216514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.898752</td>\n",
       "      <td>0.890594</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.050604</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.507905</td>\n",
       "      <td>0.196330</td>\n",
       "      <td>0.282952</td>\n",
       "      <td>0.196330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62750</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.889981</td>\n",
       "      <td>0.882831</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.023833</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.519655</td>\n",
       "      <td>0.110092</td>\n",
       "      <td>0.178306</td>\n",
       "      <td>0.110092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.895052</td>\n",
       "      <td>0.885587</td>\n",
       "      <td>0.915116</td>\n",
       "      <td>0.036565</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.486539</td>\n",
       "      <td>0.148624</td>\n",
       "      <td>0.223382</td>\n",
       "      <td>0.148624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7cbf74-7324-4298-af0f-8237d9647d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"boundEstimatorModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d3d82-4a42-4f1c-88cb-15758c4a60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundModel2 = BoundEstimatorModel.from_pretrained(\"boundEstimatorModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381622b1-6fa0-4a33-b6f0-03f5efadb812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
