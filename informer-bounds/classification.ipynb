{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcae1152-8587-4463-ab97-0e5bc899cc23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('common_10s_20231112213000.csv', parse_dates=[\"date\"]).drop(['Unnamed: 0'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21a3c24-8adc-4562-8fd3-615c015c02fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20009502 entries, 0 to 20009501\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   ticker    object        \n",
      " 1   date      datetime64[ns]\n",
      " 2   open      float64       \n",
      " 3   high      float64       \n",
      " 4   low       float64       \n",
      " 5   close     float64       \n",
      " 6   average   float64       \n",
      " 7   volume    int64         \n",
      " 8   barcount  int64         \n",
      "dtypes: datetime64[ns](1), float64(5), int64(2), object(1)\n",
      "memory usage: 1.3+ GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>average</th>\n",
       "      <th>volume</th>\n",
       "      <th>barcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2023-09-01 09:30:00</td>\n",
       "      <td>453.17</td>\n",
       "      <td>453.20</td>\n",
       "      <td>452.89</td>\n",
       "      <td>452.90</td>\n",
       "      <td>453.064</td>\n",
       "      <td>397591</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2023-09-01 09:30:10</td>\n",
       "      <td>452.91</td>\n",
       "      <td>453.01</td>\n",
       "      <td>452.89</td>\n",
       "      <td>452.95</td>\n",
       "      <td>452.956</td>\n",
       "      <td>77825</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2023-09-01 09:30:20</td>\n",
       "      <td>452.95</td>\n",
       "      <td>453.09</td>\n",
       "      <td>452.95</td>\n",
       "      <td>453.02</td>\n",
       "      <td>453.034</td>\n",
       "      <td>47865</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2023-09-01 09:30:30</td>\n",
       "      <td>453.01</td>\n",
       "      <td>453.13</td>\n",
       "      <td>452.90</td>\n",
       "      <td>452.91</td>\n",
       "      <td>453.001</td>\n",
       "      <td>53428</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2023-09-01 09:30:40</td>\n",
       "      <td>452.92</td>\n",
       "      <td>453.10</td>\n",
       "      <td>452.91</td>\n",
       "      <td>453.03</td>\n",
       "      <td>452.981</td>\n",
       "      <td>65112</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker                date    open    high     low   close  average  volume  \\\n",
       "0    SPY 2023-09-01 09:30:00  453.17  453.20  452.89  452.90  453.064  397591   \n",
       "1    SPY 2023-09-01 09:30:10  452.91  453.01  452.89  452.95  452.956   77825   \n",
       "2    SPY 2023-09-01 09:30:20  452.95  453.09  452.95  453.02  453.034   47865   \n",
       "3    SPY 2023-09-01 09:30:30  453.01  453.13  452.90  452.91  453.001   53428   \n",
       "4    SPY 2023-09-01 09:30:40  452.92  453.10  452.91  453.03  452.981   65112   \n",
       "\n",
       "   barcount  \n",
       "0       944  \n",
       "1       584  \n",
       "2       312  \n",
       "3       408  \n",
       "4       423  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290abd48-21b7-4535-a9c4-b04ccf20f74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 55 dataframes grouped by tickers\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 363960 entries, 8268907 to 19377701\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count   Dtype         \n",
      "---  ------    --------------   -----         \n",
      " 0   date      363960 non-null  datetime64[ns]\n",
      " 1   open      363960 non-null  float64       \n",
      " 2   high      363960 non-null  float64       \n",
      " 3   low       363960 non-null  float64       \n",
      " 4   close     363960 non-null  float64       \n",
      " 5   average   363960 non-null  float64       \n",
      " 6   volume    363960 non-null  int64         \n",
      " 7   barcount  363960 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(5), int64(2)\n",
      "memory usage: 25.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>average</th>\n",
       "      <th>volume</th>\n",
       "      <th>barcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8268907</th>\n",
       "      <td>2023-03-27 09:30:00</td>\n",
       "      <td>13.96</td>\n",
       "      <td>14.03</td>\n",
       "      <td>13.95</td>\n",
       "      <td>14.00</td>\n",
       "      <td>13.978</td>\n",
       "      <td>246106</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268908</th>\n",
       "      <td>2023-03-27 09:30:10</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.015</td>\n",
       "      <td>11512</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268909</th>\n",
       "      <td>2023-03-27 09:30:20</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.018</td>\n",
       "      <td>13960</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268910</th>\n",
       "      <td>2023-03-27 09:30:30</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.013</td>\n",
       "      <td>18475</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268911</th>\n",
       "      <td>2023-03-27 09:30:40</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.020</td>\n",
       "      <td>32586</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date   open   high    low  close  average  volume  \\\n",
       "8268907 2023-03-27 09:30:00  13.96  14.03  13.95  14.00   13.978  246106   \n",
       "8268908 2023-03-27 09:30:10  14.00  14.02  14.00  14.02   14.015   11512   \n",
       "8268909 2023-03-27 09:30:20  14.02  14.02  14.01  14.02   14.018   13960   \n",
       "8268910 2023-03-27 09:30:30  14.02  14.03  14.01  14.01   14.013   18475   \n",
       "8268911 2023-03-27 09:30:40  14.02  14.03  14.00  14.03   14.020   32586   \n",
       "\n",
       "         barcount  \n",
       "8268907       158  \n",
       "8268908        23  \n",
       "8268909        23  \n",
       "8268910        93  \n",
       "8268911        63  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by ticker and remove the ticker label\n",
    "df_bytickers = [ticker.drop(columns=['ticker']) for _, ticker in df.groupby(df.ticker)]\n",
    "print(f\"There is {len(df_bytickers)} dataframes grouped by tickers\")\n",
    "df_bytickers[0].info()\n",
    "df_bytickers[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0813882e-4722-44da-b3b3-cc5e883f48a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is roughly 156 dataframes that correspond to days for each ticker\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2340 entries, 8268907 to 8271246\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   date      2340 non-null   float64\n",
      " 1   open      2340 non-null   float64\n",
      " 2   high      2340 non-null   float64\n",
      " 3   low       2340 non-null   float64\n",
      " 4   close     2340 non-null   float64\n",
      " 5   average   2340 non-null   float64\n",
      " 6   volume    2340 non-null   int64  \n",
      " 7   barcount  2340 non-null   int64  \n",
      "dtypes: float64(6), int64(2)\n",
      "memory usage: 164.5 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>average</th>\n",
       "      <th>volume</th>\n",
       "      <th>barcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8268907</th>\n",
       "      <td>9.500000</td>\n",
       "      <td>13.96</td>\n",
       "      <td>14.03</td>\n",
       "      <td>13.95</td>\n",
       "      <td>14.00</td>\n",
       "      <td>13.978</td>\n",
       "      <td>246106</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268908</th>\n",
       "      <td>9.502778</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.015</td>\n",
       "      <td>11512</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268909</th>\n",
       "      <td>9.505556</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.018</td>\n",
       "      <td>13960</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268910</th>\n",
       "      <td>9.508333</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.013</td>\n",
       "      <td>18475</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268911</th>\n",
       "      <td>9.511111</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.03</td>\n",
       "      <td>14.020</td>\n",
       "      <td>32586</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date   open   high    low  close  average  volume  barcount\n",
       "8268907  9.500000  13.96  14.03  13.95  14.00   13.978  246106       158\n",
       "8268908  9.502778  14.00  14.02  14.00  14.02   14.015   11512        23\n",
       "8268909  9.505556  14.02  14.02  14.01  14.02   14.018   13960        23\n",
       "8268910  9.508333  14.02  14.03  14.01  14.01   14.013   18475        93\n",
       "8268911  9.511111  14.02  14.03  14.00  14.03   14.020   32586        63"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping by day, and reformatting the date to be time of day instead of datetime\n",
    "df_bydate = [[date for _, date in dates.groupby(dates['date'].dt.date)] for dates in df_bytickers]\n",
    "df_bydate = [[date.apply(lambda x: x.dt.hour + x.dt.minute/60 + x.dt.second/3600 if x.name in ['date'] else x) for date in ticker] for ticker in df_bydate]\n",
    "print(f\"There is roughly {len(df_bydate[0])} dataframes that correspond to days for each ticker\")\n",
    "df_bydate[0][0].info()\n",
    "df_bydate[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83962140-20aa-4e14-8389-3e5565fe56ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_with_zeros = df.eq(0).any()[lambda x: x].keys().values\n",
    "eps = 1e-16\n",
    "df_deltas = [[date.apply(lambda x: x + eps if x.name in columns_with_zeros else x) for date in ticker] for ticker in df_bydate]\n",
    "df_deltas = [[date.apply(lambda x: x.pct_change() if x.name not in ['date'] else x).iloc[1:] for date in ticker] for ticker in df_deltas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1492611-6947-45b7-8d60-676ade99c521",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2339 entries, 8268908 to 8271246\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   date      2339 non-null   float64\n",
      " 1   open      2339 non-null   float64\n",
      " 2   high      2339 non-null   float64\n",
      " 3   low       2339 non-null   float64\n",
      " 4   close     2339 non-null   float64\n",
      " 5   average   2339 non-null   float64\n",
      " 6   volume    2339 non-null   float64\n",
      " 7   barcount  2339 non-null   float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 164.5 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>average</th>\n",
       "      <th>volume</th>\n",
       "      <th>barcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8268908</th>\n",
       "      <td>9.502778</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>0.003584</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>-0.953223</td>\n",
       "      <td>-0.854430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268909</th>\n",
       "      <td>9.505556</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.212648</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268910</th>\n",
       "      <td>9.508333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.323424</td>\n",
       "      <td>3.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268911</th>\n",
       "      <td>9.511111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000714</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.763789</td>\n",
       "      <td>-0.322581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268912</th>\n",
       "      <td>9.513889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>-0.620236</td>\n",
       "      <td>-0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      open      high       low     close   average    volume  \\\n",
       "8268908  9.502778  0.002865 -0.000713  0.003584  0.001429  0.002647 -0.953223   \n",
       "8268909  9.505556  0.001429  0.000000  0.000714  0.000000  0.000214  0.212648   \n",
       "8268910  9.508333  0.000000  0.000713  0.000000 -0.000713 -0.000357  0.323424   \n",
       "8268911  9.511111  0.000000  0.000000 -0.000714  0.001428  0.000500  0.763789   \n",
       "8268912  9.513889  0.000000  0.000713  0.001429  0.000000  0.000428 -0.620236   \n",
       "\n",
       "         barcount  \n",
       "8268908 -0.854430  \n",
       "8268909  0.000000  \n",
       "8268910  3.043478  \n",
       "8268911 -0.322581  \n",
       "8268912 -0.142857  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deltas[0][0].info()\n",
    "df_deltas[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe9c54c-a536-463a-8822-198032ae1033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.892697979016721e-05\n",
      "['date' 'open' 'high' 'low' 'close' 'average' 'volume' 'barcount']\n"
     ]
    }
   ],
   "source": [
    "df_deltas = df_deltas\n",
    "#print(df_deltas[0][0].info())\n",
    "daily_sums = list()\n",
    "for tickers in df_deltas:\n",
    "    for days in tickers:\n",
    "        daily_sums.append(days.loc[:, 'average'].sum())\n",
    "daily_average = abs(sum(daily_sums) / len(daily_sums))\n",
    "features = df_deltas[0][0].columns.values\n",
    "print(daily_average)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e81210-532e-41b8-ba7e-8753e51f4c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "data = list(itertools.chain(*df_deltas))\n",
    "data_2339 = list()\n",
    "for v in data:\n",
    "    if (len(v) == 2339):\n",
    "        data_2339.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91ccba49-297d-4efa-a7c3-cf5db5b06b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "class StockDataset(TensorDataset):\n",
    "    def __init__(self, data, known_interval_in_tens_of_seconds=720, predict_interval_in_tens_of_seconds=180, daily_interval_in_tens_of_seconds=2339):\n",
    "        self.data = data\n",
    "        self.known_interval_in_tens_of_seconds = known_interval_in_tens_of_seconds\n",
    "        self.predict_interval_in_tens_of_seconds = predict_interval_in_tens_of_seconds\n",
    "        self.daily_length = daily_interval_in_tens_of_seconds - (known_interval_in_tens_of_seconds + predict_interval_in_tens_of_seconds)\n",
    "        self.length = len(data) * self.daily_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        list_idx = index // self.daily_length\n",
    "        df_idx = index % self.daily_length\n",
    "        known_df_idx = df_idx + self.known_interval_in_tens_of_seconds\n",
    "        predict_df_idx = known_df_idx + self.predict_interval_in_tens_of_seconds\n",
    "        #past_values = self.data[list_idx][['average']].iloc[df_idx:known_df_idx].values\n",
    "        #past_time_features = self.data[list_idx].loc[:, self.data[list_idx].columns != 'average'].iloc[df_idx:known_df_idx].values\n",
    "        #future_time_features = self.data[list_idx].loc[:, self.data[list_idx].columns != 'average'].iloc[known_df_idx:predict_df_idx].values\n",
    "        #future_values = self.data[list_idx][['average']].iloc[known_df_idx:predict_df_idx].values\n",
    "        #past_observed_mask = np.ones(past_values.shape)\n",
    "        return {\"past_values\": self.data[list_idx].loc[:, self.data[list_idx].columns != 'date'].iloc[df_idx:known_df_idx].values, \"future_values\": self.data[list_idx].loc[:, self.data[list_idx].columns != 'date'].iloc[known_df_idx:predict_df_idx].values, \n",
    "                \"past_time_features\": self.data[list_idx][['date']].iloc[df_idx:known_df_idx].values, \"future_time_features\": self.data[list_idx][['date']].iloc[known_df_idx:predict_df_idx].values}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e512a7-ebd1-4949-a809-cd2732402f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = StockDataset(data_2339)\n",
    "print(dataset[0]['past_time_features'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7935c738-101a-4a15-b4ec-fb8ff33e3f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds, eval_ds, test_ds, l = torch.utils.data.random_split(dataset, [0.4, 0.0005, 0.2, 0.3995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "901104a9-ad11-4b8d-b633-802d49b5ae73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import InformerConfig, InformerForPrediction, Trainer, TrainingArguments, DefaultDataCollator\n",
    "from transformers.utils import is_sagemaker_mp_enabled\n",
    "from evaluate import load\n",
    "from torch import nn\n",
    "\n",
    "mase_metric = load(\"evaluate-metric/mase\", \"multilist\")\n",
    "smape_metric = load(\"evaluate-metric/smape\", \"multilist\")\n",
    "\n",
    "alpha = 2\n",
    "\n",
    "configuration = InformerConfig(\n",
    "    context_length=720-7,\n",
    "    prediction_length=180,\n",
    "    input_size=7,\n",
    "    num_time_features=1)\n",
    "\n",
    "model = InformerForPrediction.from_pretrained(\"forecasting_model_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf010c6-bc8c-428f-96ef-5916f94d652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "class BoundEstimatorConfig(PretrainedConfig):\n",
    "    model_type = \"informer\"\n",
    "    attribute_map = {\n",
    "        \"hidden_size\": \"d_model\",\n",
    "        \"num_attention_heads\": \"encoder_attention_heads\",\n",
    "        \"num_hidden_layers\": \"encoder_layers\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_length = None,\n",
    "        context_length = None,\n",
    "        distribution_output = \"student_t\",\n",
    "        loss = \"nll\",\n",
    "        input_size = 1,\n",
    "        lags_sequence = None,\n",
    "        scaling = \"mean\",\n",
    "        num_dynamic_real_features = 0,\n",
    "        num_static_real_features = 0,\n",
    "        num_static_categorical_features = 0,\n",
    "        num_time_features = 0,\n",
    "        cardinality = None,\n",
    "        embedding_dimension = None,\n",
    "        d_model = 64,\n",
    "        encoder_ffn_dim = 32,\n",
    "        encoder_attention_heads = 2,\n",
    "        encoder_layers = 2,\n",
    "        is_encoder_decoder = False,\n",
    "        activation_function  = \"gelu\",\n",
    "        dropout = 0.05,\n",
    "        encoder_layerdrop = 0.1,\n",
    "        attention_dropout = 0.1,\n",
    "        activation_dropout = 0.1,\n",
    "        num_parallel_samples = 100,\n",
    "        init_std = 0.02,\n",
    "        use_cache = True,\n",
    "        # Informer arguments\n",
    "        attention_type = \"prob\",\n",
    "        sampling_factor = 5,\n",
    "        distil = True,\n",
    "        num_labels = 3,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # time series specific configuration\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length or prediction_length\n",
    "        self.distribution_output = distribution_output\n",
    "        self.loss = loss\n",
    "        self.input_size = input_size\n",
    "        self.num_time_features = num_time_features\n",
    "        self.lags_sequence = lags_sequence if lags_sequence is not None else [1, 2, 3, 4, 5, 6, 7]\n",
    "        self.scaling = scaling\n",
    "        self.num_dynamic_real_features = num_dynamic_real_features\n",
    "        self.num_static_real_features = num_static_real_features\n",
    "        self.num_static_categorical_features = num_static_categorical_features\n",
    "\n",
    "        # set cardinality\n",
    "        if cardinality and num_static_categorical_features > 0:\n",
    "            if len(cardinality) != num_static_categorical_features:\n",
    "                raise ValueError(\n",
    "                    \"The cardinality should be a list of the same length as `num_static_categorical_features`\"\n",
    "                )\n",
    "            self.cardinality = cardinality\n",
    "        else:\n",
    "            self.cardinality = [0]\n",
    "\n",
    "        # set embedding_dimension\n",
    "        if embedding_dimension and num_static_categorical_features > 0:\n",
    "            if len(embedding_dimension) != num_static_categorical_features:\n",
    "                raise ValueError(\n",
    "                    \"The embedding dimension should be a list of the same length as `num_static_categorical_features`\"\n",
    "                )\n",
    "            self.embedding_dimension = embedding_dimension\n",
    "        else:\n",
    "            self.embedding_dimension = [min(50, (cat + 1) // 2) for cat in self.cardinality]\n",
    "\n",
    "        self.num_parallel_samples = num_parallel_samples\n",
    "\n",
    "        # Transformer architecture configuration\n",
    "        self.feature_size = input_size * len(self.lags_sequence) + self._number_of_features\n",
    "        self.d_model = d_model\n",
    "        self.encoder_attention_heads = encoder_attention_heads\n",
    "        self.encoder_ffn_dim = encoder_ffn_dim\n",
    "        self.encoder_layers = encoder_layers\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.encoder_layerdrop = encoder_layerdrop\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.init_std = init_std\n",
    "\n",
    "        self.output_attentions = False\n",
    "        self.output_hidden_states = False\n",
    "\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        # Informer\n",
    "        self.attention_type = attention_type\n",
    "        self.sampling_factor = sampling_factor\n",
    "        self.distil = distil\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n",
    "        \n",
    "    @property\n",
    "    def _number_of_features(self) -> int:\n",
    "        return (\n",
    "            sum(self.embedding_dimension)\n",
    "            + self.num_dynamic_real_features\n",
    "            + self.num_time_features\n",
    "            + self.num_static_real_features\n",
    "            + self.input_size * 2  # the log1p(abs(loc)) and log(scale) features\n",
    "        )\n",
    "    def from_informer_config(self, config):\n",
    "        # time series specific configuration\n",
    "        self.prediction_length = config.prediction_length\n",
    "        self.context_length = config.context_length\n",
    "        self.distribution_output = config.distribution_output\n",
    "        self.loss = config.loss\n",
    "        self.input_size = config.input_size\n",
    "        self.num_time_features = config.num_time_features\n",
    "        self.lags_sequence = config.lags_sequence\n",
    "        self.scaling = config.scaling\n",
    "        self.num_dynamic_real_features = config.num_dynamic_real_features\n",
    "        self.num_static_real_features = config.num_static_real_features\n",
    "        self.num_static_categorical_features = config.num_static_categorical_features\n",
    "\n",
    "        # set cardinality\n",
    "        self.cardinality = config.cardinality\n",
    "\n",
    "        # set embedding_dimension\n",
    "        self.embedding_dimension = config.embedding_dimension\n",
    "\n",
    "        self.num_parallel_samples = config.num_parallel_samples\n",
    "\n",
    "        # Transformer architecture configuration\n",
    "        self.feature_size = config.feature_size\n",
    "        self.d_model = config.d_model\n",
    "        self.encoder_attention_heads = config.encoder_attention_heads\n",
    "        self.encoder_ffn_dim = config.encoder_ffn_dim\n",
    "        self.encoder_layers = config.encoder_layers\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "        self.encoder_layerdrop = config.encoder_layerdrop\n",
    "\n",
    "        self.activation_function = config.activation_function\n",
    "        self.init_std = config.init_std\n",
    "\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_attentions\n",
    "\n",
    "        self.use_cache = config.use_cache\n",
    "\n",
    "        # Informer\n",
    "        self.attention_type = config.attention_type\n",
    "        self.sampling_factor = config.sampling_factor\n",
    "        self.distil = config.distil\n",
    "        self.num_labels = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd727250-9409-490f-adc7-142e1ef410eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "class InformerFeatureEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed a sequence of categorical features.\n",
    "    Args: \n",
    "        cardinalities (`list[int]`):\n",
    "            List of cardinalities of the categorical features.\n",
    "        embedding_dims (`list[int]`):\n",
    "            List of embedding dimensions of the categorical features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cardinalities: List[int], embedding_dims: List[int]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = len(cardinalities)\n",
    "        self.embedders = nn.ModuleList([nn.Embedding(c, d) for c, d in zip(cardinalities, embedding_dims)])\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.num_features > 1:\n",
    "            # we slice the last dimension, giving an array of length\n",
    "            # self.num_features with shape (N,T) or (N)\n",
    "            cat_feature_slices = torch.chunk(features, self.num_features, dim=-1)\n",
    "        else:\n",
    "            cat_feature_slices = [features]\n",
    "\n",
    "        return torch.cat(\n",
    "            [\n",
    "                embed(cat_feature_slice.squeeze(-1))\n",
    "                for embed, cat_feature_slice in zip(self.embedders, cat_feature_slices)\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesStdScaler with TimeSeries->Informer\n",
    "class InformerStdScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Standardize features by calculating the mean and scaling along some given dimension `dim`, and then normalizes it\n",
    "    by subtracting from the mean and dividing by the standard deviation.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to calculate the mean and standard deviation.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "        minimum_scale (`float`, *optional*, defaults to 1e-5):\n",
    "            Default scale that is used for elements that are constantly zero along dimension `dim`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, keepdim: bool = False, minimum_scale: float = 1e-5):\n",
    "        super().__init__()\n",
    "        if not dim > 0:\n",
    "            raise ValueError(\"Cannot compute scale along dim = 0 (batch dimension), please provide dim > 0\")\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "        self.minimum_scale = minimum_scale\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: torch.Tensor, weights: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        denominator = weights.sum(self.dim, keepdim=self.keepdim)\n",
    "        denominator = denominator.clamp_min(1.0)\n",
    "        loc = (data * weights).sum(self.dim, keepdim=self.keepdim) / denominator\n",
    "\n",
    "        variance = (((data - loc) * weights) ** 2).sum(self.dim, keepdim=self.keepdim) / denominator\n",
    "        scale = torch.sqrt(variance + self.minimum_scale)\n",
    "        return (data - loc) / scale, loc, scale\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesMeanScaler with TimeSeries->Informer\n",
    "class InformerMeanScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a scaling factor as the weighted average absolute value along dimension `dim`, and scales the data\n",
    "    accordingly.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to compute the scale.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "        default_scale (`float`, *optional*, defaults to `None`):\n",
    "            Default scale that is used for elements that are constantly zero. If `None`, we use the scale of the batch.\n",
    "        minimum_scale (`float`, *optional*, defaults to 1e-10):\n",
    "            Default minimum possible scale that is used for any item.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dim: int = -1, keepdim: bool = True, default_scale: Optional[float] = None, minimum_scale: float = 1e-10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "        self.minimum_scale = minimum_scale\n",
    "        self.default_scale = default_scale\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: torch.Tensor, observed_indicator: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # shape: (N, [C], T=1)\n",
    "        ts_sum = (data * observed_indicator).abs().sum(self.dim, keepdim=True)\n",
    "        num_observed = observed_indicator.sum(self.dim, keepdim=True)\n",
    "\n",
    "        scale = ts_sum / torch.clamp(num_observed, min=1)\n",
    "\n",
    "        # If `default_scale` is provided, we use it, otherwise we use the scale\n",
    "        # of the batch.\n",
    "        if self.default_scale is None:\n",
    "            batch_sum = ts_sum.sum(dim=0)\n",
    "            batch_observations = torch.clamp(num_observed.sum(0), min=1)\n",
    "            default_scale = torch.squeeze(batch_sum / batch_observations)\n",
    "        else:\n",
    "            default_scale = self.default_scale * torch.ones_like(scale)\n",
    "\n",
    "        # apply default scale where there are no observations\n",
    "        scale = torch.where(num_observed > 0, scale, default_scale)\n",
    "\n",
    "        # ensure the scale is at least `self.minimum_scale`\n",
    "        scale = torch.clamp(scale, min=self.minimum_scale)\n",
    "        scaled_data = data / scale\n",
    "\n",
    "        if not self.keepdim:\n",
    "            scale = scale.squeeze(dim=self.dim)\n",
    "\n",
    "        return scaled_data, torch.zeros_like(scale), scale\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesNOPScaler with TimeSeries->Informer\n",
    "class InformerNOPScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Assigns a scaling factor equal to 1 along dimension `dim`, and therefore applies no scaling to the input data.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to compute the scale.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, keepdim: bool = False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "\n",
    "    def forward(\n",
    "        self, data: torch.Tensor, observed_indicator: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        scale = torch.ones_like(data, requires_grad=False).mean(dim=self.dim, keepdim=self.keepdim)\n",
    "        loc = torch.zeros_like(data, requires_grad=False).mean(dim=self.dim, keepdim=self.keepdim)\n",
    "        return data, loc, scale\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.weighted_average\n",
    "def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the weighted average of a given tensor across a given `dim`, masking values associated with weight zero,\n",
    "    meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n",
    "    Args:\n",
    "        input_tensor (`torch.FloatTensor`):\n",
    "            Input tensor, of which the average must be computed.\n",
    "        weights (`torch.FloatTensor`, *optional*):\n",
    "            Weights tensor, of the same shape as `input_tensor`.\n",
    "        dim (`int`, *optional*):\n",
    "            The dim along which to average `input_tensor`.\n",
    "    Returns:\n",
    "        `torch.FloatTensor`: The tensor with values averaged along the specified `dim`.\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        weighted_tensor = torch.where(weights != 0, input_tensor * weights, torch.zeros_like(input_tensor))\n",
    "        sum_weights = torch.clamp(weights.sum(dim=dim) if dim else weights.sum(), min=1.0)\n",
    "        return (weighted_tensor.sum(dim=dim) if dim else weighted_tensor.sum()) / sum_weights\n",
    "    else:\n",
    "        return input_tensor.mean(dim=dim)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.nll\n",
    "def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the negative log likelihood loss from input distribution with respect to target.\n",
    "    \"\"\"\n",
    "    return -input.log_prob(target)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart._make_causal_mask\n",
    "def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n",
    "    \"\"\"\n",
    "    Make causal mask used for bi-directional self-attention.\n",
    "    \"\"\"\n",
    "    bsz, tgt_len = input_ids_shape\n",
    "    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
    "    mask_cond = torch.arange(mask.size(-1))\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "    mask = mask.to(dtype)\n",
    "\n",
    "    if past_key_values_length > 0:\n",
    "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n",
    "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart._expand_mask\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian->Informer\n",
    "class InformerSinusoidalPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
    "\n",
    "    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n",
    "        super().__init__(num_positions, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weight(out: nn.Parameter) -> nn.Parameter:\n",
    "        \"\"\"\n",
    "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
    "        the 2nd half of the vector. [dim // 2:]\n",
    "        \"\"\"\n",
    "        n_pos, dim = out.shape\n",
    "        position_enc = np.array(\n",
    "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
    "        )\n",
    "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
    "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
    "        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "        out.detach_()\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n",
    "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = input_ids_shape[:2]\n",
    "        positions = torch.arange(\n",
    "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
    "        )\n",
    "        return super().forward(positions)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesValueEmbedding with TimeSeries->Info\n",
    "class InformerValueEmbedding(nn.Module):\n",
    "    def __init__(self, feature_size, d_model):\n",
    "        super().__init__()\n",
    "        self.value_projection = nn.Linear(in_features=feature_size, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.value_projection(x)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Informer\n",
    "class InformerAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "        # the provided `key_value_states` to support prefix tuning\n",
    "        if (\n",
    "            is_cross_attention\n",
    "            and past_key_value is not None\n",
    "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "        ):\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.reshape(*proj_shape)\n",
    "        value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned across GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value\n",
    "\n",
    "\n",
    "class InformerProbSparseAttention(nn.Module):\n",
    "    \"\"\"Probabilistic Attention mechanism to select the \"active\"\n",
    "    queries rather than the \"lazy\" queries and provides a sparse Transformer thus mitigating the quadratic compute and\n",
    "    memory requirements of vanilla attention\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        sampling_factor: int = 5,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.factor = sampling_factor\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "        # the provided `key_value_states` to support prefix tuning\n",
    "        if (\n",
    "            is_cross_attention\n",
    "            and past_key_value is not None\n",
    "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "        ):\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.reshape(*proj_shape)\n",
    "        value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "        key_states_time_length = key_states.size(1)  # L_K\n",
    "        log_key_states_time_length = np.ceil(np.log1p(key_states_time_length)).astype(\"int\").item()  # log_L_K\n",
    "\n",
    "        query_states_time_length = query_states.size(1)  # L_Q\n",
    "        log_query_states_time_length = np.ceil(np.log1p(query_states_time_length)).astype(\"int\").item()  # log_L_Q\n",
    "\n",
    "        u_part = min(self.factor * query_states_time_length * log_key_states_time_length, key_states_time_length)\n",
    "        u = min(self.factor * log_query_states_time_length, query_states_time_length)\n",
    "\n",
    "        if key_states_time_length > 0:\n",
    "            index_sample = torch.randint(0, key_states_time_length, (u_part,))\n",
    "            k_sample = key_states[:, index_sample, :]\n",
    "        else:\n",
    "            k_sample = key_states\n",
    "\n",
    "        queries_keys_sample = torch.bmm(query_states, k_sample.transpose(1, 2))  # Q_K_sampled\n",
    "\n",
    "        # find the Top_k query with sparsity measurement\n",
    "        if u > 0:\n",
    "            sparsity_measurement = queries_keys_sample.max(dim=-1)[0] - torch.div(\n",
    "                queries_keys_sample.sum(dim=-1), key_states_time_length\n",
    "            )  # M\n",
    "            top_u_sparsity_measurement = sparsity_measurement.topk(u, sorted=False)[1]  # M_top\n",
    "\n",
    "            # calculate q_reduce: query_states[:, top_u_sparsity_measurement]\n",
    "            dim_for_slice = torch.arange(query_states.size(0)).unsqueeze(-1)\n",
    "            q_reduce = query_states[dim_for_slice, top_u_sparsity_measurement]\n",
    "        else:\n",
    "            q_reduce = query_states\n",
    "            top_u_sparsity_measurement = None\n",
    "\n",
    "        # Use q_reduce to calculate attention weights\n",
    "        attn_weights = torch.bmm(q_reduce, key_states.transpose(1, 2))\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        if attn_weights.size() != (bsz * self.num_heads, u, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, u, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            prob_mask = attention_mask.expand(bsz, self.num_heads, tgt_len, src_len).reshape(\n",
    "                bsz * self.num_heads, tgt_len, src_len\n",
    "            )\n",
    "\n",
    "            if top_u_sparsity_measurement is not None:\n",
    "                dim_for_slice = torch.arange(prob_mask.size(0)).unsqueeze(-1)\n",
    "                prob_mask = prob_mask[dim_for_slice, top_u_sparsity_measurement, :]\n",
    "\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, u, src_len) + prob_mask.view(\n",
    "                bsz, self.num_heads, u, src_len\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, u, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, u, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, u, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        # calculate context for updating the attn_output, based on:\n",
    "        # https://github.com/zhouhaoyi/Informer2020/blob/ac59c7447135473fb2aafeafe94395f884d5c7a5/models/attn.py#L74\n",
    "        if self.is_decoder:\n",
    "            context = value_states.cumsum(dim=-2)\n",
    "        else:\n",
    "            v_mean_dim_time = value_states.mean(dim=-2)\n",
    "            context = (\n",
    "                v_mean_dim_time.unsqueeze(dim=1)\n",
    "                .expand(bsz * self.num_heads, query_states_time_length, v_mean_dim_time.size(-1))\n",
    "                .clone()\n",
    "            )\n",
    "\n",
    "        if top_u_sparsity_measurement is not None:\n",
    "            # update context: copy the attention output to the context at top_u_sparsity_measurement index\n",
    "            dim_for_slice = torch.arange(context.size(0)).unsqueeze(-1)\n",
    "            context[dim_for_slice, top_u_sparsity_measurement, :] = attn_output\n",
    "            attn_output = context\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned across GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value\n",
    "\n",
    "\n",
    "# source: https://github.com/zhouhaoyi/Informer2020/blob/main/models/encoder.py\n",
    "class InformerConvLayer(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        self.downConv = nn.Conv1d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=c_in,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode=\"circular\",\n",
    "        )\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x.permute(0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: InformerConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "        if config.attention_type == \"prob\":\n",
    "            self.self_attn = InformerProbSparseAttention(\n",
    "                embed_dim=self.embed_dim,\n",
    "                num_heads=config.encoder_attention_heads,\n",
    "                dropout=config.attention_dropout,\n",
    "                sampling_factor=config.sampling_factor,\n",
    "            )\n",
    "        else:\n",
    "            self.self_attn = InformerAttention(\n",
    "                embed_dim=self.embed_dim,\n",
    "                num_heads=config.encoder_attention_heads,\n",
    "                dropout=config.attention_dropout,\n",
    "            )\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        attention_mask: torch.FloatTensor,\n",
    "        layer_head_mask: torch.FloatTensor,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        hidden_states, attn_weights, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        if hidden_states.dtype == torch.float16 and (\n",
    "            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n",
    "        ):\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "class BoundEstimatorPreTrainedModel(PreTrainedModel):\n",
    "    config_class = BoundEstimatorConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    main_input_name = \"past_values\"\n",
    "    supports_gradient_checkpointing = True\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.init_std\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, InformerDecoder):\n",
    "            module.gradient_checkpointing = value\n",
    "            \n",
    "class InformerEncoder(BoundEstimatorPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Informer encoder consisting of *config.encoder_layers* self attention layers with distillation layers. Each\n",
    "    attention layer is an [`InformerEncoderLayer`].\n",
    "    Args:\n",
    "        config: InformerConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BoundEstimatorPreTrainedModel):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.encoder_layerdrop\n",
    "        self.gradient_checkpointing = False\n",
    "        if config.prediction_length is None:\n",
    "            raise ValueError(\"The `prediction_length` config needs to be specified.\")\n",
    "\n",
    "        self.value_embedding = InformerValueEmbedding(feature_size=config.feature_size, d_model=config.d_model)\n",
    "        self.embed_positions = InformerSinusoidalPositionalEmbedding(\n",
    "            config.context_length + config.prediction_length, config.d_model\n",
    "        )\n",
    "        self.layers = nn.ModuleList([InformerEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        if config.distil:\n",
    "            self.conv_layers = nn.ModuleList(\n",
    "                    [InformerConvLayer(config.d_model) for _ in range(config.encoder_layers - 1)]\n",
    "                )\n",
    "            self.conv_layers.append(None)\n",
    "        else:\n",
    "            self.conv_layers = [None] * config.encoder_layers\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        hidden_states = self.value_embedding(inputs_embeds)\n",
    "        embed_pos = self.embed_positions(inputs_embeds.size())\n",
    "\n",
    "        hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            if head_mask.size()[0] != (len(self.layers)):\n",
    "                raise ValueError(\n",
    "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "\n",
    "        for idx, (encoder_layer, conv_layer) in enumerate(zip(self.layers, self.conv_layers)):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(encoder_layer),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        (head_mask[idx] if head_mask is not None else None),\n",
    "                    )\n",
    "                    if conv_layer is not None:\n",
    "                        output = torch.utils.checkpoint.checkpoint(conv_layer, layer_outputs[0])\n",
    "                        layer_outputs = (output,) + layer_outputs[1:]\n",
    "                else:\n",
    "                    layer_outputs = encoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                        output_attentions=output_attentions,\n",
    "                    )\n",
    "                    if conv_layer is not None:\n",
    "                        output = conv_layer(layer_outputs[0])\n",
    "                        layer_outputs = (output,) + layer_outputs[1:]\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef83df87-8eb5-441c-95a8-0af228e151ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "            \n",
    "class BoundEstimatorModel(BoundEstimatorPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        if config.scaling == \"mean\" or config.scaling:\n",
    "            self.scaler = InformerMeanScaler(dim=1, keepdim=True)\n",
    "        elif config.scaling == \"std\":\n",
    "            self.scaler = InformerStdScaler(dim=1, keepdim=True)\n",
    "        else:\n",
    "            self.scaler = InformerNOPScaler(dim=1, keepdim=True)\n",
    "\n",
    "        # transformer encoder and mask initializer\n",
    "        self.encoder = InformerEncoder(config)\n",
    "        \n",
    "        self.classifier = nn.Linear(config.d_model, config.num_labels)\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def from_informer_model(self, model):\n",
    "        self.encoder = model.encoder\n",
    "        self.scaler = model.scaler\n",
    "        \n",
    "    @property\n",
    "    def _past_length(self):\n",
    "        return self.config.context_length + max(self.config.lags_sequence)\n",
    "    \n",
    "    def get_lagged_subsequences(\n",
    "        self, sequence: torch.Tensor, subsequences_length: int, shift: int = 0\n",
    "    ):\n",
    "        sequence_length = sequence.shape[1]\n",
    "        indices = [lag - shift for lag in self.config.lags_sequence]\n",
    "\n",
    "        if max(indices) + subsequences_length > sequence_length:\n",
    "            raise ValueError(\n",
    "                f\"lags cannot go further than history length, found lag {max(indices)} \"\n",
    "                f\"while history length is only {sequence_length}\"\n",
    "            )\n",
    "\n",
    "        lagged_values = []\n",
    "        for lag_index in indices:\n",
    "            begin_index = -lag_index - subsequences_length\n",
    "            end_index = -lag_index if lag_index > 0 else None\n",
    "            lagged_values.append(sequence[:, begin_index:end_index, ...])\n",
    "        return torch.stack(lagged_values, dim=-1)\n",
    "    \n",
    "    def create_network_inputs(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        past_observed_mask = None,\n",
    "        future_values = None,\n",
    "        future_time_features = None,\n",
    "    ):\n",
    "        # time feature\n",
    "        time_feat = (\n",
    "            torch.cat(\n",
    "                (\n",
    "                    past_time_features[:, self._past_length - self.config.context_length :, ...],\n",
    "                    future_time_features,\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            if future_values is not None\n",
    "            else past_time_features[:, self._past_length - self.config.context_length :, ...]\n",
    "        )\n",
    "\n",
    "        # target\n",
    "        if past_observed_mask is None:\n",
    "            past_observed_mask = torch.ones_like(past_values)\n",
    "\n",
    "        context = past_values[:, -self.config.context_length :]\n",
    "        observed_context = past_observed_mask[:, -self.config.context_length :]\n",
    "        _, loc, scale = self.scaler(context, observed_context)\n",
    "\n",
    "        inputs = (\n",
    "            (torch.cat((past_values, future_values), dim=1) - loc) / scale\n",
    "            if future_values is not None\n",
    "            else (past_values - loc) / scale\n",
    "        )\n",
    "\n",
    "        # static features\n",
    "        log_abs_loc = loc.abs().log1p() if self.config.input_size == 1 else loc.squeeze(1).abs().log1p()\n",
    "        log_scale = scale.log() if self.config.input_size == 1 else scale.squeeze(1).log()\n",
    "        static_feat = torch.cat((log_abs_loc, log_scale), dim=1)\n",
    "\n",
    "        expanded_static_feat = static_feat.unsqueeze(1).expand(-1, time_feat.shape[1], -1)\n",
    "\n",
    "        # all features\n",
    "        features = torch.cat((expanded_static_feat, time_feat), dim=-1)\n",
    "\n",
    "        # lagged features\n",
    "        subsequences_length = (\n",
    "            self.config.context_length + self.config.prediction_length\n",
    "            if future_values is not None\n",
    "            else self.config.context_length\n",
    "        )\n",
    "        lagged_sequence = self.get_lagged_subsequences(sequence=inputs, subsequences_length=subsequences_length)\n",
    "        lags_shape = lagged_sequence.shape\n",
    "        reshaped_lagged_sequence = lagged_sequence.reshape(lags_shape[0], lags_shape[1], -1)\n",
    "\n",
    "        if reshaped_lagged_sequence.shape[1] != time_feat.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"input length {reshaped_lagged_sequence.shape[1]} and time feature lengths {time_feat.shape[1]} does not match\"\n",
    "            )\n",
    "\n",
    "        # transformer inputs\n",
    "        transformer_inputs = torch.cat((reshaped_lagged_sequence, features), dim=-1)\n",
    "\n",
    "        return transformer_inputs, loc, scale, static_feat       \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        past_observed_mask: torch.Tensor,\n",
    "        future_values = None,\n",
    "        future_time_features = None,\n",
    "        decoder_attention_mask = None,\n",
    "        head_mask = None,\n",
    "        decoder_head_mask = None,\n",
    "        cross_attn_head_mask= None,\n",
    "        past_key_values = None,\n",
    "        output_hidden_states= None,\n",
    "        output_attentions = None,\n",
    "        use_cache = None,\n",
    "        return_dict = None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_inputs, loc, scale, static_feat = self.create_network_inputs(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            future_values=future_values,\n",
    "            future_time_features=future_time_features,\n",
    "        )\n",
    "\n",
    "        enc_input = transformer_inputs[:, : self.config.context_length, ...]\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=enc_input,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        outputs = self.classifier(torch.mean(encoder_outputs.last_hidden_state, dim=1))\n",
    "        if not return_dict:\n",
    "            return {\"outputs\": outputs } + encoder_outputs + (loc, scale, static_feat)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae5b216a-beb7-4fa0-9fd7-37a4c01b3a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = BoundEstimatorConfig()\n",
    "config.from_informer_config(model.config)\n",
    "boundModel = BoundEstimatorModel(config)\n",
    "boundModel.from_informer_model(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e7de7d0-5479-4ad0-9a2b-7057ca988b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='steps',\n",
    "    report_to=\"none\",\n",
    "    logging_steps=20_000)\n",
    "\n",
    "\n",
    "class TimeSerieDataCollator:\n",
    "    def __init__(self):\n",
    "        self.default_data_collator = DefaultDataCollator()\n",
    " \n",
    "    def __call__(self, batch):\n",
    "        [x.update({'past_observed_mask': torch.ones(x[\"past_values\"].shape)}) for x in batch]\n",
    "        return self.default_data_collator(batch)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    _, results = eval_pred\n",
    "    predictions = np.argmax(results[\"actual\"], -1)\n",
    "    labels = results[\"expected\"]\n",
    "    non_zeros_labels = labels[labels != 0]\n",
    "    non_zeros_predictions = predictions[labels != 0]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', warn_for=tuple())\n",
    "    nz_precision, nz_recall, nz_f1, _ = precision_recall_fscore_support(non_zeros_labels, non_zeros_predictions, average='weighted', warn_for=tuple())\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    nz_acc = accuracy_score(non_zeros_labels, non_zeros_predictions)\n",
    "    \n",
    "    return { 'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall, \"nonzeros_pred\": np.count_nonzero(predictions) / len(predictions), \"nonzeros_labels\": np.count_nonzero(labels) / len(labels),\n",
    "            'nz_precision': nz_precision, 'nz_recall': nz_recall, 'nz_f1': nz_f1, 'nz_accuracy': nz_acc }\n",
    "\n",
    "\n",
    "class StockTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # forward pass\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        outputs = model(**inputs)\n",
    "        label = torch.zeros((inputs[\"future_values\"].size(0))).long()\n",
    "        average = torch.zeros((inputs[\"future_values\"].size(0)))\n",
    "        for i, v in enumerate(inputs[\"future_values\"]):\n",
    "            for j in v:\n",
    "                average[i] += j[features.tolist().index(\"average\") - 1].detach().cpu()\n",
    "                if average[i] >= (0.01 + daily_average):\n",
    "                    label[i] = 2\n",
    "                elif average[i] <= -(0.01 + daily_average):\n",
    "                    label[i] = 1\n",
    "        label = label.to(model.device)\n",
    "        loss = loss(outputs, label)\n",
    "        return (loss, outputs.encoder_last_hidden_state) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys= None):\n",
    "        label = torch.zeros((inputs[\"future_values\"].size(0))).long()\n",
    "        average = torch.zeros((inputs[\"future_values\"].size(0)))\n",
    "        for i, v in enumerate(inputs[\"future_values\"]):\n",
    "            for j in v:\n",
    "                average[i] += j[features.tolist().index(\"average\") - 1].detach().cpu()\n",
    "                if average[i] >= (0.01 + daily_average):\n",
    "                    label[i] = 2\n",
    "                elif average[i] <= -(0.01 + daily_average):\n",
    "                    label[i] = 1\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return (None, list(), { \"actual\": F.softmax(outputs, dim=1).detach().cpu(), \"expected\": label })\n",
    "\n",
    "\n",
    "trainer = StockTrainer(\n",
    "    model=boundModel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=TimeSerieDataCollator(),\n",
    "    compute_metrics=compute_metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908ef1d-a6f0-4484-b63e-b70b58b4dbca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='447272' max='1225022' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 447272/1225022 13:35:29 < 23:38:03, 9.14 it/s, Epoch 0.73/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Nonzeros Pred</th>\n",
       "      <th>Nonzeros Labels</th>\n",
       "      <th>Nz Precision</th>\n",
       "      <th>Nz Recall</th>\n",
       "      <th>Nz F1</th>\n",
       "      <th>Nz Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.908750</td>\n",
       "      <td>0.883373</td>\n",
       "      <td>0.874688</td>\n",
       "      <td>0.908750</td>\n",
       "      <td>0.032648</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.465132</td>\n",
       "      <td>0.118674</td>\n",
       "      <td>0.174705</td>\n",
       "      <td>0.118674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.322500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.908260</td>\n",
       "      <td>0.889286</td>\n",
       "      <td>0.876337</td>\n",
       "      <td>0.908260</td>\n",
       "      <td>0.046033</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.379679</td>\n",
       "      <td>0.146597</td>\n",
       "      <td>0.206901</td>\n",
       "      <td>0.146597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.318700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.904995</td>\n",
       "      <td>0.894930</td>\n",
       "      <td>0.894364</td>\n",
       "      <td>0.904995</td>\n",
       "      <td>0.073131</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.516490</td>\n",
       "      <td>0.253054</td>\n",
       "      <td>0.299083</td>\n",
       "      <td>0.253054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.890798</td>\n",
       "      <td>0.880866</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.039993</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.514717</td>\n",
       "      <td>0.158813</td>\n",
       "      <td>0.239807</td>\n",
       "      <td>0.158813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910545</td>\n",
       "      <td>0.889445</td>\n",
       "      <td>0.886330</td>\n",
       "      <td>0.910545</td>\n",
       "      <td>0.043421</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.169284</td>\n",
       "      <td>0.226058</td>\n",
       "      <td>0.169284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.889395</td>\n",
       "      <td>0.867413</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.039830</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.284243</td>\n",
       "      <td>0.172775</td>\n",
       "      <td>0.214915</td>\n",
       "      <td>0.172775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.312900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.890686</td>\n",
       "      <td>0.880269</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.045054</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.507777</td>\n",
       "      <td>0.186736</td>\n",
       "      <td>0.224587</td>\n",
       "      <td>0.186736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.310500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.889485</td>\n",
       "      <td>0.878655</td>\n",
       "      <td>0.912994</td>\n",
       "      <td>0.037218</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.483199</td>\n",
       "      <td>0.160558</td>\n",
       "      <td>0.216167</td>\n",
       "      <td>0.160558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.906954</td>\n",
       "      <td>0.897135</td>\n",
       "      <td>0.889413</td>\n",
       "      <td>0.906954</td>\n",
       "      <td>0.067581</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.497933</td>\n",
       "      <td>0.244328</td>\n",
       "      <td>0.314868</td>\n",
       "      <td>0.244328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.304600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.897257</td>\n",
       "      <td>0.889113</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.043911</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.552591</td>\n",
       "      <td>0.197208</td>\n",
       "      <td>0.290454</td>\n",
       "      <td>0.197208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>0.304800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909892</td>\n",
       "      <td>0.897892</td>\n",
       "      <td>0.896080</td>\n",
       "      <td>0.909892</td>\n",
       "      <td>0.065948</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.531407</td>\n",
       "      <td>0.254799</td>\n",
       "      <td>0.312775</td>\n",
       "      <td>0.254799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.908586</td>\n",
       "      <td>0.891828</td>\n",
       "      <td>0.882084</td>\n",
       "      <td>0.908586</td>\n",
       "      <td>0.059256</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.418618</td>\n",
       "      <td>0.207679</td>\n",
       "      <td>0.236463</td>\n",
       "      <td>0.207679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.890098</td>\n",
       "      <td>0.881736</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.530248</td>\n",
       "      <td>0.150087</td>\n",
       "      <td>0.225998</td>\n",
       "      <td>0.150087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>0.300700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.897399</td>\n",
       "      <td>0.888792</td>\n",
       "      <td>0.914626</td>\n",
       "      <td>0.045544</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.525323</td>\n",
       "      <td>0.198953</td>\n",
       "      <td>0.283751</td>\n",
       "      <td>0.198953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300000</td>\n",
       "      <td>0.298500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.899321</td>\n",
       "      <td>0.891239</td>\n",
       "      <td>0.912831</td>\n",
       "      <td>0.053869</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.524620</td>\n",
       "      <td>0.221640</td>\n",
       "      <td>0.311546</td>\n",
       "      <td>0.221640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909892</td>\n",
       "      <td>0.889169</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.909892</td>\n",
       "      <td>0.049298</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.487738</td>\n",
       "      <td>0.181501</td>\n",
       "      <td>0.210727</td>\n",
       "      <td>0.181501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340000</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911525</td>\n",
       "      <td>0.894623</td>\n",
       "      <td>0.884522</td>\n",
       "      <td>0.911525</td>\n",
       "      <td>0.047502</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.465444</td>\n",
       "      <td>0.178010</td>\n",
       "      <td>0.255369</td>\n",
       "      <td>0.178010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360000</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911525</td>\n",
       "      <td>0.893824</td>\n",
       "      <td>0.885138</td>\n",
       "      <td>0.911525</td>\n",
       "      <td>0.050930</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.482696</td>\n",
       "      <td>0.193717</td>\n",
       "      <td>0.248202</td>\n",
       "      <td>0.193717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380000</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.889931</td>\n",
       "      <td>0.880690</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.036076</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.458779</td>\n",
       "      <td>0.153578</td>\n",
       "      <td>0.214903</td>\n",
       "      <td>0.153578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400000</td>\n",
       "      <td>0.291900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911525</td>\n",
       "      <td>0.891598</td>\n",
       "      <td>0.882914</td>\n",
       "      <td>0.911525</td>\n",
       "      <td>0.042279</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.478192</td>\n",
       "      <td>0.167539</td>\n",
       "      <td>0.239352</td>\n",
       "      <td>0.167539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420000</td>\n",
       "      <td>0.296600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909729</td>\n",
       "      <td>0.893204</td>\n",
       "      <td>0.882658</td>\n",
       "      <td>0.909729</td>\n",
       "      <td>0.055175</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.445598</td>\n",
       "      <td>0.200698</td>\n",
       "      <td>0.248916</td>\n",
       "      <td>0.200698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440000</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.888667</td>\n",
       "      <td>0.880341</td>\n",
       "      <td>0.911688</td>\n",
       "      <td>0.040973</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.552405</td>\n",
       "      <td>0.162304</td>\n",
       "      <td>0.205197</td>\n",
       "      <td>0.162304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b7cbf74-7324-4298-af0f-8237d9647d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"boundEstimatorModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d3d82-4a42-4f1c-88cb-15758c4a60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundModel2 = BoundEstimatorModel.from_pretrained(\"boundEstimatorModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381622b1-6fa0-4a33-b6f0-03f5efadb812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
